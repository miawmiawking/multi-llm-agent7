import requests
import streamlit as st
from langchain.tools import DuckDuckGoSearchRun
import PyPDF2
from docx import Document
import pandas as pd
import chardet
import base64
import io
from langchain.docstore.document import Document as LC_Document # æ–°å¢ langchain ç›¸å…³ä¾èµ–
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS  # æ›¿æ¢ Chroma
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFaceHub
import tempfile
import os
from pydub import AudioSegment
from openai import OpenAI
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS
import re
from urllib.parse import urlparse
import pickle
from langchain_community.embeddings import HuggingFaceEmbeddings
from datetime import datetime

# å…¨å±€å˜é‡å®šä¹‰
CHROMADB_PATH = None
COLLECTION_NAME = "rag_collection"

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ ä¼šè¯ç®¡ç†ç›¸å…³çš„åˆå§‹åŒ–
if "chat_history" not in st.session_state:
    st.session_state.chat_history = {}  # ç”¨äºå­˜å‚¨ä¸åŒæ¨¡å‹çš„å¯¹è¯å†å²

def manage_chat_history(model_type, role, content):
    """ç®¡ç†å¯¹è¯å†å²"""
    if model_type not in st.session_state.chat_history:
        st.session_state.chat_history[model_type] = []
    
    # æ·»åŠ æ–°æ¶ˆæ¯
    st.session_state.chat_history[model_type].append({
        "role": role,
        "content": content,
        "timestamp": datetime.now().isoformat()
    })
    
    # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…ï¼ˆæœ€è¿‘10è½®å¯¹è¯ï¼‰
    max_history = 10
    if len(st.session_state.chat_history[model_type]) > max_history * 2:  # æ¯è½®åŒ…å«ç”¨æˆ·å’ŒåŠ©æ‰‹çš„æ¶ˆæ¯
        st.session_state.chat_history[model_type] = st.session_state.chat_history[model_type][-max_history * 2:]

def get_chat_history(model_type):
    """è·å–æŒ‡å®šæ¨¡å‹çš„å¯¹è¯å†å²"""
    return st.session_state.chat_history.get(model_type, [])

def format_messages_for_model(model_type, current_prompt):
    """æ ¹æ®ä¸åŒæ¨¡å‹æ ¼å¼åŒ–æ¶ˆæ¯å†å²"""
    messages = []
    
    # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
    if st.session_state.selected_assistant:
        domain = next(k for k, v in st.session_state.assistant_market.items() 
                    if st.session_state.selected_assistant in v)
        role_prompt = st.session_state.assistant_market[domain][st.session_state.selected_assistant]
        system_message = {
            "role": "system",
            "content": f"{role_prompt}\n\nè¯·ä»¥{st.session_state.selected_assistant}çš„èº«ä»½å›ç­”é—®é¢˜ï¼Œä¿æŒå¯¹è¯è¿è´¯æ€§ã€‚"
        }
    else:
        system_message = {
            "role": "system",
            "content": "You are a helpful assistant. Please maintain conversation coherence."
        }
    
    messages.append(system_message)
    
    # æ·»åŠ å†å²æ¶ˆæ¯
    history = get_chat_history(model_type)
    for msg in history:
        messages.append({
            "role": msg["role"],
            "content": msg["content"]
        })
    
    # æ·»åŠ å½“å‰é—®é¢˜
    messages.append({
        "role": "user",
        "content": current_prompt
    })
    
    return messages

# ChromaDB é…ç½®å‡½æ•°
def configure_chromadb():
    """é…ç½® ChromaDB å­˜å‚¨è·¯å¾„"""
    st.divider()
    with st.expander("ğŸ—„ï¸ RAGçŸ¥è¯†åº“è®¾ç½®ä¸ç®¡ç†", expanded=not bool(st.session_state.get("chromadb_path"))):
        st.markdown("### å‘é‡æ•°æ®åº“å­˜å‚¨è·¯å¾„")
        
        # é»˜è®¤è·¯å¾„è®¾ç½®
        default_path = os.path.join(os.path.expanduser("~"), "chromadb_data")
        
        # æ˜¾ç¤ºå½“å‰è·¯å¾„
        current_path = st.session_state.get("chromadb_path", "")
        if current_path:
            st.info(f"å½“å‰è·¯å¾„ï¼š{current_path}")
        
        # è·¯å¾„è¾“å…¥
        new_path = st.text_input(
            "è®¾ç½®å­˜å‚¨è·¯å¾„",
            value=current_path or default_path,
            placeholder="ä¾‹å¦‚ï¼š/Users/YourName/Documents/chromadb",
            key="chromadb_path_input"
        )
        
        # ç¡®è®¤æŒ‰é’®
        if st.button("âœ… ç¡®è®¤è·¯å¾„"):
            try:
                # ç¡®ä¿è·¯å¾„å­˜åœ¨
                os.makedirs(new_path, exist_ok=True)
                
                # æµ‹è¯•è·¯å¾„æ˜¯å¦å¯å†™
                test_file = os.path.join(new_path, "test_write.txt")
                try:
                    with open(test_file, "w") as f:
                        f.write("test")
                    os.remove(test_file)
                except Exception as e:
                    st.error(f"è·¯å¾„æ— å†™å…¥æƒé™ï¼š{str(e)}")
                    return
                
                # æ›´æ–°è·¯å¾„
                st.session_state.chromadb_path = new_path
                st.success("âœ… å‘é‡æ•°æ®åº“è·¯å¾„è®¾ç½®æˆåŠŸï¼")
                
            except Exception as e:
                st.error(f"è·¯å¾„è®¾ç½®å¤±è´¥ï¼š{str(e)}")
        
        # æ¸…ç©ºçŸ¥è¯†åº“æŒ‰é’®
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºçŸ¥è¯†åº“"):
            if clear_vector_store():
                st.success("âœ… çŸ¥è¯†åº“å·²æ¸…ç©º")
                st.rerun()
        
        st.markdown("""
        **è¯´æ˜ï¼š**
        1. é¦–æ¬¡ä½¿ç”¨è¯·è®¾ç½®å­˜å‚¨è·¯å¾„
        2. è·¯å¾„éœ€è¦æœ‰å†™å…¥æƒé™
        3. å»ºè®®é€‰æ‹©æœ¬åœ°å›ºå®šä½ç½®
        4. ç¡®ä¿æœ‰è¶³å¤Ÿå­˜å‚¨ç©ºé—´
        """)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []
if "search_enabled" not in st.session_state:
    st.session_state.search_enabled = False
if "file_analyzed" not in st.session_state:
    st.session_state.file_analyzed = False
if "file_content" not in st.session_state:
    st.session_state.file_content = ""
if "file_summary" not in st.session_state:
    st.session_state.file_summary = ""
if "selected_model" not in st.session_state:
    st.session_state.selected_model = "è±†åŒ…"
if "selected_function" not in st.session_state:
    st.session_state.selected_function = "æ™ºèƒ½é—®ç­”"
if "api_keys" not in st.session_state:
    st.session_state.api_keys = {}
if "rag_enabled" not in st.session_state:
    st.session_state.rag_enabled = False
if "rag_data" not in st.session_state:
    st.session_state.rag_data = []
if "temperature" not in st.session_state:
    st.session_state.temperature = 0.5
if "max_tokens" not in st.session_state:
    st.session_state.max_tokens = 2048
if "chromadb_path" not in st.session_state:
    st.session_state.chromadb_path = ""
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "selected_assistant" not in st.session_state:
    st.session_state.selected_assistant = None
if "assistant_market" not in st.session_state:
    st.session_state.assistant_market = {
        "é‡‘èé¢†åŸŸ": {
            "è´¢åŠ¡åˆ†æå¸ˆ": """[è§’è‰²æŒ‡å—] ä¸“ä¸šè´¢åŠ¡åˆ†æå¸ˆï¼Œä¸“é•¿é¢†åŸŸï¼š
1. è´¢åŠ¡æŠ¥è¡¨åˆ†æå’Œè§£è¯»
2. è´¢åŠ¡é£é™©è¯„ä¼°
3. ä¼ä¸šä¼°å€¼å’Œè´¢åŠ¡å»ºæ¨¡
4. è¡Œä¸šå¯¹æ ‡åˆ†æ
æä¾›ä¸¥è°¨ã€ä¸“ä¸šçš„è´¢åŠ¡åˆ†æå»ºè®®ï¼Œæ³¨é‡æ•°æ®æ”¯æŒå’Œåˆ†æä¾æ®ã€‚""",
            
            "æŠ•èµ„ç­–ç•¥ä¸“å®¶": """[è§’è‰²æŒ‡å—] èµ„æ·±æŠ•èµ„ç­–ç•¥ä¸“å®¶ï¼Œæ ¸å¿ƒèƒ½åŠ›ï¼š
1. å®è§‚ç»æµåˆ†æ
2. èµ„äº§é…ç½®ç­–ç•¥
3. æŠ•èµ„ç»„åˆç®¡ç†
4. é£é™©æ”¶ç›Šè¯„ä¼°
åŸºäºä¸“ä¸šçŸ¥è¯†æä¾›æ·±å…¥çš„æŠ•èµ„è§è§£ã€‚""",
            
            "ç†è´¢è§„åˆ’å¸ˆ": """[è§’è‰²æŒ‡å—] ä¸“ä¸šç†è´¢è§„åˆ’å¸ˆï¼Œä¸“æ³¨é¢†åŸŸï¼š
1. ä¸ªäººè´¢åŠ¡è§„åˆ’
2. é€€ä¼‘è®¡åˆ’åˆ¶å®š
3. ä¿é™©é…ç½®å»ºè®®
4. ç¨åŠ¡ç­¹åˆ’ä¼˜åŒ–
æ ¹æ®å®¢æˆ·æƒ…å†µæä¾›ä¸ªæ€§åŒ–ç†è´¢æ–¹æ¡ˆã€‚""",
            
            "æŠ•èµ„é¡¾é—®ä¸“å®¶": """[è§’è‰²æŒ‡å—] ä¸“ä¸šæŠ•èµ„é¡¾é—®ï¼Œæ“…é•¿æ–¹å‘ï¼š
1. æŠ•èµ„äº§å“åˆ†æ
2. å¸‚åœºè¶‹åŠ¿ç ”åˆ¤
3. æŠ•èµ„æœºä¼šè¯†åˆ«
4. é£é™©æ§åˆ¶ç­–ç•¥
æä¾›ä¸“ä¸šã€è´Ÿè´£ä»»çš„æŠ•èµ„å»ºè®®ã€‚""",
            
            "è‚¡ç¥¨åˆ†æä¸“å®¶": """[è§’è‰²æŒ‡å—] ä¸“ä¸šè‚¡ç¥¨åˆ†æä¸“å®¶ï¼Œç ”ç©¶é¢†åŸŸï¼š
1. æŠ€æœ¯é¢åˆ†æ
2. åŸºæœ¬é¢åˆ†æ
3. è¡Œä¸šç ”ç©¶
4. ä¸ªè‚¡ç ”ç©¶
æä¾›ä¸“ä¸šçš„è‚¡å¸‚åˆ†æè§‚ç‚¹ã€‚"""
        },
        "ITé¢†åŸŸ": {
            "æ•°æ®ç§‘å­¦å®¶": """[è§’è‰²æŒ‡å—] èµ„æ·±æ•°æ®ç§‘å­¦å®¶ï¼Œä¸“ä¸šé¢†åŸŸï¼š
1. æ•°æ®åˆ†æå’ŒæŒ–æ˜
2. æœºå™¨å­¦ä¹ ç®—æ³•
3. ç»Ÿè®¡å»ºæ¨¡
4. æ•°æ®å¯è§†åŒ–
ä¸“æ³¨æ•°æ®ç§‘å­¦è§£å†³æ–¹æ¡ˆã€‚""",
            
            "å…¨æ ˆç¨‹åºå‘˜": """[è§’è‰²æŒ‡å—] èµ„æ·±å…¨æ ˆå·¥ç¨‹å¸ˆï¼ŒæŠ€æœ¯æ ˆï¼š
1. å‰ç«¯å¼€å‘(HTML/CSS/JavaScript)
2. åç«¯å¼€å‘(Python/Java/Node.js)
3. æ•°æ®åº“è®¾è®¡
4. ç³»ç»Ÿæ¶æ„
æä¾›å®Œæ•´çš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆã€‚""",
            
            "ITæ¶æ„å¸ˆ": """[è§’è‰²æŒ‡å—] ä¸“ä¸šITæ¶æ„å¸ˆï¼Œæ ¸å¿ƒèƒ½åŠ›ï¼š
1. ç³»ç»Ÿæ¶æ„è®¾è®¡
2. æŠ€æœ¯é€‰å‹å†³ç­–
3. æ€§èƒ½ä¼˜åŒ–
4. å®‰å…¨æ¶æ„
ä¸“æ³¨ä¼ä¸šçº§æ¶æ„è®¾è®¡ã€‚""",
            
            "Promptå·¥ç¨‹å¸ˆ": """[è§’è‰²æŒ‡å—] AIäº¤äº’ä¸“å®¶ï¼Œä¸“é•¿ï¼š
1. æç¤ºè¯ä¼˜åŒ–è®¾è®¡
2. AIäº¤äº’ç­–ç•¥
3. ä¸Šä¸‹æ–‡ç®¡ç†
4. è¾“å‡ºè´¨é‡æ§åˆ¶
ä¸“æ³¨AIäº¤äº’æ•ˆæœä¼˜åŒ–ã€‚""",
            
            "æ•°æ®åº“ç®¡ç†ä¸“å®¶": """[è§’è‰²æŒ‡å—] èµ„æ·±æ•°æ®åº“ä¸“å®¶ï¼Œä¸“ä¸šæ–¹å‘ï¼š
1. æ•°æ®åº“è®¾è®¡ä¼˜åŒ–
2. æ€§èƒ½è°ƒä¼˜
3. æ•°æ®å®‰å…¨ç®¡ç†
4. å¤‡ä»½æ¢å¤ç­–ç•¥
æä¾›æ•°æ®åº“ä¸“ä¸šè§£å†³æ–¹æ¡ˆã€‚"""
        },
        "å•†ä¸šé¢†åŸŸ": {
            "äº§å“ç»ç†": """[è§’è‰²æŒ‡å—] èµ„æ·±äº§å“ç»ç†ï¼Œä¸“ä¸šé¢†åŸŸï¼š
1. äº§å“æˆ˜ç•¥è§„åˆ’
2. ç”¨æˆ·éœ€æ±‚åˆ†æ
3. äº§å“ç”Ÿå‘½å‘¨æœŸç®¡ç†
4. äº§å“è·¯çº¿å›¾åˆ¶å®š
5. è·¨å›¢é˜Ÿåä½œç®¡ç†
ä¸“æ³¨äº§å“æˆ˜ç•¥ä¸è½åœ°ã€‚""",
            
            "ä¾›åº”é“¾ç­–ç•¥ä¸“å®¶": """[è§’è‰²æŒ‡å—] ä¾›åº”é“¾ç®¡ç†ä¸“å®¶ï¼Œæ ¸å¿ƒèƒ½åŠ›ï¼š
1. ä¾›åº”é“¾ä¼˜åŒ–è®¾è®¡
2. åº“å­˜ç®¡ç†ç­–ç•¥
3. ç‰©æµç½‘ç»œè§„åˆ’
4. ä¾›åº”å•†ç®¡ç†
5. é£é™©è¯„ä¼°ä¸æ§åˆ¶
æä¾›ä¸“ä¸šä¾›åº”é“¾è§£å†³æ–¹æ¡ˆã€‚""",
            
            "æ•°å­—è¥é”€ä¸“å®¶": """[è§’è‰²æŒ‡å—] æ•°å­—è¥é”€ä¸“å®¶ï¼Œä¸“é•¿é¢†åŸŸï¼š
1. æ•°å­—è¥é”€ç­–ç•¥åˆ¶å®š
2. ç¤¾äº¤åª’ä½“è¥é”€
3. å†…å®¹è¥é”€ç­–ç•¥
4. ç”¨æˆ·å¢é•¿ç­–ç•¥
5. ROIåˆ†æä¸ä¼˜åŒ–
ä¸“æ³¨æ•°å­—è¥é”€æ•ˆæœæå‡ã€‚""",
            
            "äººåŠ›ä¸“å®¶": """[è§’è‰²æŒ‡å—] äººåŠ›èµ„æºä¸“å®¶ï¼Œä¸“ä¸šæ–¹å‘ï¼š
1. äººæ‰æ‹›è˜ä¸åŸ¹å…»
2. ç»©æ•ˆç®¡ç†ä½“ç³»
3. è–ªé…¬ç¦åˆ©è®¾è®¡
4. ç»„ç»‡å‘å±•è§„åˆ’
5. å‘˜å·¥å…³ç³»ç®¡ç†
æä¾›ä¸“ä¸šäººåŠ›èµ„æºè§£å†³æ–¹æ¡ˆã€‚""",
            
            "ç¤¾äº¤åª’ä½“ç»ç†": """[è§’è‰²æŒ‡å—] ç¤¾äº¤åª’ä½“è¿è¥ä¸“å®¶ï¼Œæ ¸å¿ƒèƒ½åŠ›ï¼š
1. ç¤¾äº¤åª’ä½“ç­–ç•¥è§„åˆ’
2. å†…å®¹åˆ›ä½œä¸ç®¡ç†
3. ç¤¾åŒºè¿è¥ä¸äº’åŠ¨
4. èˆ†æƒ…ç›‘æµ‹ä¸å±æœºå¤„ç†
5. å½±å“åŠ›æ•°æ®åˆ†æ
ä¸“æ³¨ç¤¾äº¤åª’ä½“æ•ˆæœä¼˜åŒ–ã€‚"""
        },
        "å’¨è¯¢é¢†åŸŸ": {
            "éº¦è‚¯é”¡é¡¾é—®": """[è§’è‰²æŒ‡å—] æˆ˜ç•¥å’¨è¯¢ä¸“å®¶ï¼Œä¸“ä¸šé¢†åŸŸï¼š
1. æˆ˜ç•¥å’¨è¯¢
2. ç»„ç»‡è½¬å‹
3. è¿è¥ä¼˜åŒ–
4. æ•°å­—åŒ–è½¬å‹
5. å•†ä¸šæ¨¡å¼åˆ›æ–°
é‡‡ç”¨ä¸“ä¸šå’¨è¯¢æ–¹æ³•è®ºã€‚""",
            
            "è¡Œä¸šè°ƒç ”ä¸“å®¶": """[è§’è‰²æŒ‡å—] è¡Œä¸šç ”ç©¶ä¸“å®¶ï¼Œç ”ç©¶æ–¹å‘ï¼š
1. å¸‚åœºè§„æ¨¡æµ‹ç®—
2. ç«äº‰æ ¼å±€åˆ†æ
3. äº§ä¸šé“¾ç ”ç©¶
4. å‘å±•è¶‹åŠ¿é¢„æµ‹
5. å•†ä¸šæœºä¼šè¯†åˆ«
æä¾›æ·±åº¦è¡Œä¸šæ´å¯Ÿã€‚""",
            
            "æˆ˜ç•¥åˆ†æå¸ˆ": """[è§’è‰²æŒ‡å—] æˆ˜ç•¥åˆ†æä¸“å®¶ï¼Œæ ¸å¿ƒèƒ½åŠ›ï¼š
1. æˆ˜ç•¥è§„åˆ’åˆ¶å®š
2. å•†ä¸šæ¨¡å¼åˆ†æ
3. å¸‚åœºè¿›å…¥ç­–ç•¥
4. ç«äº‰æˆ˜ç•¥åˆ†æ
5. æˆ˜ç•¥å®æ–½è·¯å¾„
ä¸“æ³¨æˆ˜ç•¥è§„åˆ’ä¸æ‰§è¡Œã€‚""",
            
            "ä¼ä¸šç­–ç•¥ä¸“å®¶": """[è§’è‰²æŒ‡å—] ä¼ä¸šæˆ˜ç•¥ä¸“å®¶ï¼Œä¸“ä¸šæ–¹å‘ï¼š
1. ä¼ä¸šå‘å±•æˆ˜ç•¥
2. ä¸šåŠ¡ç»„åˆä¼˜åŒ–
3. æŠ•èµ„å¹¶è´­ç­–ç•¥
4. é£é™©ç®¡ç†ç­–ç•¥
5. ç»„ç»‡å˜é©ç®¡ç†
æä¾›å…¨é¢æˆ˜ç•¥å»ºè®®ã€‚""",
            
            "æ³•å¾‹é¡¾é—®ä¸“å®¶": """[è§’è‰²æŒ‡å—] å•†ä¸šæ³•å¾‹ä¸“å®¶ï¼Œä¸“ä¸šé¢†åŸŸï¼š
1. å•†ä¸šåˆåŒå®¡æŸ¥
2. çŸ¥è¯†äº§æƒä¿æŠ¤
3. ä¼ä¸šåˆè§„ç®¡ç†
4. é£é™©æ³•å¾‹è¯„ä¼°
5. äº‰è®®è§£å†³æ–¹æ¡ˆ
æä¾›ä¸“ä¸šæ³•å¾‹å’¨è¯¢ã€‚"""
        },
        "å­¦æœ¯é¢†åŸŸ": {
            "ç»æµå­¦æ•™æˆ": """[è§’è‰²æŒ‡å—] èµ„æ·±ç»æµå­¦æ•™æˆï¼Œç ”ç©¶é¢†åŸŸï¼š
1. å®è§‚ç»æµç†è®º
2. ç»æµæ”¿ç­–åˆ†æ
3. å›½é™…ç»æµå­¦
4. å‘å±•ç»æµå­¦
5. è¡Œä¸ºç»æµå­¦
ä¸“æ³¨ç»æµå­¦ç†è®ºç ”ç©¶ä¸å®è¯åˆ†æï¼Œæä¾›ä¸¥è°¨çš„å­¦æœ¯è§è§£ã€‚""",
            
            "é‡‘èå­¦æ•™æˆ": """[è§’è‰²æŒ‡å—] èµ„æ·±é‡‘èå­¦æ•™æˆï¼Œä¸“ä¸šæ–¹å‘ï¼š
1. å…¬å¸é‡‘èç†è®º
2. èµ„äº§å®šä»·æ¨¡å‹
3. é‡‘èå¸‚åœºç ”ç©¶
4. é‡‘èå·¥ç¨‹å­¦
5. è¡Œä¸ºé‡‘èå­¦
æä¾›æ·±å…¥çš„é‡‘èå­¦ç†è®ºåˆ†æä¸ç ”ç©¶æ–¹æ³•æŒ‡å¯¼ã€‚""",
            
            "ç»Ÿè®¡å­¦å®¶": """[è§’è‰²æŒ‡å—] ä¸“ä¸šç»Ÿè®¡å­¦å®¶ï¼Œç ”ç©¶é¢†åŸŸï¼š
1. æ•°ç†ç»Ÿè®¡
2. å®éªŒè®¾è®¡
3. å¤šå…ƒç»Ÿè®¡åˆ†æ
4. æ—¶é—´åºåˆ—åˆ†æ
5. è´å¶æ–¯ç»Ÿè®¡
ä¸“æ³¨ç»Ÿè®¡æ–¹æ³•è®ºä¸æ•°æ®åˆ†æï¼Œæä¾›ä¸¥è°¨çš„ç»Ÿè®¡å»ºè®®ã€‚""",
            
            "å†å²å­¦å®¶": """[è§’è‰²æŒ‡å—] èµ„æ·±å†å²å­¦å®¶ï¼Œç ”ç©¶æ–¹å‘ï¼š
1. å†å²äº‹ä»¶åˆ†æ
2. å†å²æ–‡çŒ®è€ƒè¯
3. å†å²æ¯”è¾ƒç ”ç©¶
4. æ–‡åŒ–å²ç ”ç©¶
5. ç»æµå²ç ”ç©¶
æä¾›ä¸“ä¸šçš„å†å²å­¦è§†è§’ä¸ç ”ç©¶æ–¹æ³•æŒ‡å¯¼ã€‚""",
            
            "æœŸåˆŠå®¡ç¨¿äºº": """[è§’è‰²æŒ‡å—] ä¸“ä¸šå­¦æœ¯æœŸåˆŠå®¡ç¨¿äººï¼Œé’ˆå¯¹æäº¤çš„å­¦æœ¯è®ºæ–‡è¿›è¡Œå…¨é¢è¯„å®¡ï¼Œé‡ç‚¹å…³æ³¨ï¼š
1. ç ”ç©¶æ–¹æ³•è¯„ä¼°ï¼šåˆ†ææ‰€é‡‡ç”¨æ–¹æ³•çš„é€‚ç”¨æ€§ä¸ä¸¥è°¨æ€§ï¼›è¯„ä¼°æ•°æ®æ”¶é›†ä¸åˆ†æè¿‡ç¨‹çš„å¯é æ€§
2. å­¦æœ¯åˆ›æ–°æ€§åˆ¤æ–­ï¼šåˆ¤æ–­ç ”ç©¶çš„æ–°é¢–æ€§åŠå…¶åœ¨é¢†åŸŸå†…çš„ç‹¬ç‰¹è´¡çŒ®ï¼›è¯„ä¼°ç ”ç©¶æ˜¯å¦å¡«è¡¥ç°æœ‰çŸ¥è¯†ç©ºç™½æˆ–æå‡ºæ–°çš„ç†è®ºæ¡†æ¶
3. æ–‡çŒ®ç»¼è¿°å®¡æŸ¥ï¼šæ£€æŸ¥æ–‡çŒ®å¼•ç”¨çš„å…¨é¢æ€§ã€ç›¸å…³æ€§åŠæ–°é¢–æ€§ï¼›è¯„ä¼°æ–‡çŒ®ç»¼è¿°æ˜¯å¦æœ‰æ•ˆæ”¯æŒç ”ç©¶èƒŒæ™¯ä¸ç›®çš„
4. å®éªŒè®¾è®¡è¯„ä»·ï¼šè¯„ä¼°å®éªŒè®¾è®¡çš„åˆç†æ€§ã€å¯é‡å¤æ€§åŠæ§åˆ¶å˜é‡çš„æœ‰æ•ˆæ€§ï¼›æ£€æŸ¥å®éªŒæ–¹æ³•æ˜¯å¦è¯¦å°½æè¿°ï¼Œä¾¿äºä»–äººå¤ç°
5. ç ”ç©¶ç»“è®ºéªŒè¯ï¼šç¡®è®¤ç»“è®ºæ˜¯å¦ç”±æ•°æ®å……åˆ†æ”¯æŒï¼Œé€»è¾‘æ˜¯å¦ä¸¥å¯†ï¼›è¯„ä¼°ç»“è®ºçš„ç§‘å­¦æ€§ä¸å®é™…åº”ç”¨ä»·å€¼
æä¾›ç»“æ„æ¸…æ™°ã€å»ºè®¾æ€§çš„æ”¹è¿›å»ºè®®ï¼ŒæŒ‡å‡ºè®ºæ–‡ä¸­çš„è¯­è¨€ã€æ ¼å¼åŠç»“æ„æ€§é—®é¢˜ï¼Œä¿æŒè¯„å®¡æ„è§å®¢è§‚ã€å…¬æ­£ã€‚""",
            
            "è¯¾é¢˜ç”³æŠ¥æŒ‡å¯¼": """[è§’è‰²æŒ‡å—] å›½å®¶ç¤¾ç§‘/è‡ªç§‘åŸºé‡‘è¯„å®¡çº§é¡¾é—®ï¼Œæä¾›å…¨æµç¨‹ç²¾ç»†åŒ–æŒ‡å¯¼ï¼š
1.é€‰é¢˜è®ºè¯ï¼šèšç„¦ç†è®ºç©ºç™½ä¸å®è·µéœ€æ±‚ï¼Œåˆ†æå›½å†…å¤–ç ”ç©¶åŠ¨æ€ï¼ŒæŒ‡å¯¼åˆ›æ–°åˆ‡å…¥ç‚¹ç­›é€‰ä¸æ ‡é¢˜æ‰“ç£¨
2.æ–¹æ¡ˆè®¾è®¡ï¼šæ„å»º"ç†è®ºæ¡†æ¶-æŠ€æœ¯è·¯çº¿-å®éªŒè®¾è®¡"ä¸‰ä½ä¸€ä½“æ–¹æ¡ˆï¼Œå¼ºè°ƒæ–¹æ³•è®ºç§‘å­¦æ€§ï¼ˆå¦‚æ··åˆç ”ç©¶è®¾è®¡ã€çºµå‘è¿½è¸ªæ¨¡å‹ï¼‰
3.æˆæœè§„åˆ’ï¼šåŒºåˆ†ç†è®ºçªç ´ï¼ˆæ–°æ¨¡å‹/èŒƒå¼ï¼‰ä¸å®è·µä»·å€¼ï¼ˆæ”¿ç­–å»ºè®®/æŠ€æœ¯åŸå‹ï¼‰ï¼Œè§„åˆ’ä¸“åˆ©/æ•°æ®åº“ç­‰å®ä½“æˆæœè½¬åŒ–è·¯å¾„
4.æ–‡æœ¬ä¼˜åŒ–ï¼šæŒ‡å¯¼æ–‡çŒ®ç»¼è¿°æ‰¹åˆ¤æ€§å†™ä½œã€æŠ€æœ¯è·¯çº¿ç”˜ç‰¹å›¾å¯è§†åŒ–ã€ç ”ç©¶åŸºç¡€ä¸è¯¾é¢˜è¡”æ¥ç­–ç•¥
5.é¢„ç®—ç¼–åˆ¶ï¼šæŒ‰è®¾å¤‡è´¹/æµ‹è¯•è´¹/ä¼šè®®è´¹åˆ†ç±»ç¼–åˆ¶ï¼ŒæŒ‡å¯¼é—´æ¥è´¹ç”¨åˆè§„æ€§æµ‹ç®—ä¸ç»©æ•ˆæ”¯å‡ºå æ¯”
6.ç­”è¾©é¢„å®¡ï¼šæ¨¡æ‹Ÿè¯„å®¡è§†è§’ï¼Œé’ˆå¯¹å­¦ç§‘ä»£ç é€‰æ‹©ã€å›¢é˜Ÿç»“æ„åˆç†æ€§ã€é¢„æœŸæˆæœå¯è¡Œæ€§ç­‰12é¡¹å¸¸è§å¦å†³ç‚¹è¿›è¡Œé£é™©è¯Šæ–­
å…¨ç¨‹æä¾›å­¦ç§‘å·®å¼‚åŒ–çš„ç”³æŠ¥ç­–ç•¥ï¼ˆå¦‚æ–‡ç§‘å¼ºè°ƒç†è®ºåˆ›æ–°ï¼Œå·¥ç§‘ä¾§é‡åº”ç”¨éªŒè¯ï¼‰ï¼ŒååŠ©æ„å»º"é—®é¢˜é©±åŠ¨-æ–¹æ³•åˆ›æ–°-ä»·å€¼é—­ç¯"çš„ç”³æŠ¥é€»è¾‘ä½“ç³»ã€‚""",
            
            "è¯¾é¢˜è¯„å®¡ä¸“å®¶": """[è§’è‰²æŒ‡å—] èµ„æ·±çš„è¯¾é¢˜è¯„å®¡ä¸“å®¶ï¼Œæ‹¥æœ‰æ·±åšçš„å­¦æœ¯èƒŒæ™¯å’Œä¸°å¯Œçš„é¡¹ç›®è¯„å®¡ç»éªŒï¼Œè¯„å®¡é‡ç‚¹ï¼š
1. é€‰é¢˜ä»·å€¼è¯„ä¼°ï¼šåˆ†æè¯¾é¢˜åœ¨å½“å‰å­¦æœ¯å‰æ²¿æˆ–å®é™…åº”ç”¨ä¸­çš„é‡è¦æ€§ã€å¿…è¦æ€§åŠå…¶æ½œåœ¨çš„ç¤¾ä¼šç»æµå½±å“
2. ç ”ç©¶æ–¹æ¡ˆå¯è¡Œæ€§ï¼šå®¡æŸ¥ç ”ç©¶è®¾è®¡çš„åˆç†æ€§ä¸ç§‘å­¦æ€§ï¼ŒåŒ…æ‹¬ç ”ç©¶æ–¹æ³•ã€æŠ€æœ¯è·¯çº¿ã€æ—¶é—´å®‰æ’åŠèµ„æºé…ç½®æ˜¯å¦åˆ‡å®å¯è¡Œ
3. åˆ›æ–°æ€§åˆ†æï¼šè¯„ä¼°è¯¾é¢˜çš„æ–°é¢–æ€§å’Œç‹¬åˆ›æ€§ï¼Œæ˜ç¡®å…¶ç›¸è¾ƒäºç°æœ‰ç ”ç©¶çš„çªç ´ç‚¹å’Œç‹¬ç‰¹è´¡çŒ®
4. ç ”ç©¶åŸºç¡€è¯„ä»·ï¼šè€ƒå¯Ÿç”³æŠ¥å›¢é˜Ÿçš„ç ”ç©¶èƒŒæ™¯ã€ä¸“ä¸šèƒ½åŠ›åŠä»¥å¾€ç›¸å…³æˆæœï¼Œç¡®ä¿å›¢é˜Ÿå…·å¤‡å®Œæˆè¯¾é¢˜çš„å®åŠ›ä¸ç»éªŒ
5. é¢„æœŸæˆæœè¯„ä¼°ï¼šé¢„æµ‹ç ”ç©¶å¯èƒ½å–å¾—çš„æˆæœåŠå…¶å­¦æœ¯ä»·å€¼æˆ–å®é™…åº”ç”¨å‰æ™¯ï¼Œè¯„ä¼°æˆæœçš„å¯æ¨å¹¿æ€§å’Œå½±å“åŠ›
è¯·åŸºäºä»¥ä¸Šè¯„å®¡é‡ç‚¹ï¼Œæä¾›è¯¦å°½ã€å®¢è§‚ä¸”å…·æœ‰å»ºè®¾æ€§çš„è¯„å®¡æ„è§ä¸æ”¹è¿›å»ºè®®ï¼Œè¯­è¨€åº”ä¸¥è°¨ä¸“ä¸šï¼Œé€»è¾‘æ¸…æ™°ã€‚"""
        }
    }

# é¡µé¢é…ç½®
st.set_page_config(page_title="å¤šæ¨¡å‹æ™ºèƒ½åŠ©æ‰‹2.10(å­¦æœ¯å¢å¼ºç‰ˆ)", layout="wide")

# åˆå§‹åŒ–/åŠ è½½ langchain å°è£…çš„ Chroma å‘é‡åº“
def get_vector_store():
    """è·å–å‘é‡æ•°æ®åº“å®ä¾‹"""
    try:
        # æ£€æŸ¥æ˜¯å¦å·²è®¾ç½®è·¯å¾„
        if not st.session_state.get("chromadb_path"):
            st.error("âš ï¸ è¯·å…ˆåœ¨ä¾§è¾¹æ è®¾ç½®å‘é‡æ•°æ®åº“å­˜å‚¨è·¯å¾„ï¼")
            return None
            
        # å¦‚æœå‘é‡åº“å·²ç»åœ¨ä¼šè¯çŠ¶æ€ä¸­ï¼Œç›´æ¥è¿”å›
        if st.session_state.get("vector_store") is not None:
            return st.session_state.vector_store
            
        # ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„è·¯å¾„
        db_path = os.path.join(st.session_state.chromadb_path, "faiss_index")
        
        # è·å– embeddings
        embeddings = get_embeddings()
        if not embeddings:
            return None
            
        # å¦‚æœå­˜åœ¨ç°æœ‰ç´¢å¼•ï¼Œåˆ™åŠ è½½
        if os.path.exists(db_path):
            try:
                vectorstore = FAISS.load_local(
                    db_path, 
                    embeddings,
                    allow_dangerous_deserialization=True  # æ·»åŠ æ­¤å‚æ•°
                )
                st.session_state.vector_store = vectorstore
                return vectorstore
            except Exception as e:
                st.error(f"åŠ è½½å‘é‡åº“å¤±è´¥ï¼š{str(e)}")
                return None
        
        # å¦‚æœä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„å‘é‡åº“å®ä¾‹
        vectorstore = FAISS.from_texts(
            texts=["åˆå§‹åŒ–æ–‡æ¡£"],
            embedding=embeddings
        )
        st.session_state.vector_store = vectorstore
        return vectorstore
        
    except Exception as e:
        st.error(f"åˆå§‹åŒ–å‘é‡åº“å¤±è´¥ï¼š{str(e)}")
        import traceback
        st.error(f"è¯¦ç»†é”™è¯¯ï¼š{traceback.format_exc()}")
        return None

# åˆå§‹åŒ– DuckDuckGo æœç´¢å·¥å…·
search_tool = DuckDuckGoSearchRun()

# æ ¸å¿ƒåŠŸèƒ½å®ç°

def handle_web_search(query):
    """è”ç½‘æœç´¢åŠŸèƒ½ï¼Œä½¿ç”¨ DuckDuckGo API"""
    if not st.session_state.search_enabled:
        return None
    try:
        search = DuckDuckGoSearchRun()
        results = search.run(query)
        return results
    except Exception as e:
        st.error(f"è”ç½‘æœç´¢å¤±è´¥: {str(e)}")
        return None

def call_model_api(prompt, model_type, rag_data=None):
    """è°ƒç”¨é™¤ RAG éƒ¨åˆ†å¤–çš„å…¶ä»–æ¥å£"""
    headers = {"Content-Type": "application/json"}
    
    try:
        # è·å–æ ¼å¼åŒ–åçš„æ¶ˆæ¯åˆ—è¡¨
        messages = format_messages_for_model(model_type, prompt)
        
        if model_type == "è±†åŒ…":
            api_key = st.session_state.api_keys.get("è±†åŒ…", "")
            if not api_key:
                st.error("è¯·æä¾›è±†åŒ… API å¯†é’¥ï¼")
                return None
            headers["Authorization"] = f"Bearer {api_key}"
            
            response = requests.post(
                "https://ark.cn-beijing.volces.com/api/v3/chat/completions",
                json={
                    "model": "ep-20250128163906-p4tb5",
                    "messages": messages,
                    "temperature": st.session_state.temperature,
                    "max_tokens": st.session_state.max_tokens
                },
                headers=headers
            )
            result = handle_response(response, rag_data)
            if result:
                manage_chat_history(model_type, "assistant", result)
            return result

        elif model_type == "DeepSeek-V3":
            api_key = st.session_state.api_keys.get("DeepSeek", "")
            if not api_key:
                st.error("è¯·æä¾› DeepSeek API å¯†é’¥ï¼")
                return None
            headers["Authorization"] = f"Bearer {api_key}"
            
            response = requests.post(
                "https://api.deepseek.com/v1/chat/completions",
                json={
                    "model": "deepseek-chat",
                    "messages": messages,
                    "temperature": st.session_state.temperature,
                    "max_tokens": st.session_state.max_tokens
                },
                headers=headers
            )
            result = handle_response(response, rag_data)
            if result:
                manage_chat_history(model_type, "assistant", result)
            return result

        elif model_type == "é€šä¹‰åƒé—®":
            api_key = st.session_state.api_keys.get("é€šä¹‰åƒé—®", "")
            if not api_key:
                st.error("è¯·æä¾› é€šä¹‰åƒé—® API å¯†é’¥ï¼")
                return None
            headers["Authorization"] = f"Bearer {api_key}"
            
            response = requests.post(
                "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions",
                json={
                    "model": "qwen-plus",
                    "messages": messages,
                    "temperature": st.session_state.temperature,
                    "max_tokens": st.session_state.max_tokens
                },
                headers=headers
            )
            result = handle_response(response, rag_data)
            if result:
                manage_chat_history(model_type, "assistant", result)
            return result

        elif model_type == "æ–‡å¿ƒä¸€è¨€":
            api_key = st.session_state.api_keys.get("æ–‡å¿ƒä¸€è¨€", "")
            if not api_key:
                st.error("è¯·æä¾› æ–‡å¿ƒä¸€è¨€ API å¯†é’¥ï¼")
                return None
            headers["Authorization"] = f"Bearer {api_key}"
            # ä¸ºæ–‡å¿ƒä¸€è¨€æ„å»ºå¢å¼ºçš„æç¤ºè¯
            enhanced_prompt = prompt
            if st.session_state.selected_assistant:
                domain = next(k for k, v in st.session_state.assistant_market.items() 
                            if st.session_state.selected_assistant in v)
                role_prompt = st.session_state.assistant_market[domain][st.session_state.selected_assistant]
                enhanced_prompt = f"{role_prompt}\n\nè¯·ä»¥{st.session_state.selected_assistant}çš„èº«ä»½å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š\n{prompt}"
            # è·å–å†å²æ¶ˆæ¯
            history = get_chat_history(model_type)
            if history:
                # å°†æœ€è¿‘çš„å¯¹è¯å†å²æ·»åŠ åˆ°æç¤ºè¯ä¸­
                recent_history = history[-6:]  # ä¿ç•™æœ€è¿‘3è½®å¯¹è¯
                history_text = "\n".join([f"{'ç”¨æˆ·' if msg['role']=='user' else 'åŠ©æ‰‹'}: {msg['content']}" 
                                        for msg in recent_history])
                enhanced_prompt = f"ä»¥ä¸‹æ˜¯å†å²å¯¹è¯ï¼š\n{history_text}\n\nå½“å‰é—®é¢˜ï¼š{enhanced_prompt}"
            response = requests.post(
                "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/completions",
                json={
                    "model": "ERNIE-Bot",
                    "messages": [{"role": "user", "content": enhanced_prompt}],
                    "temperature": st.session_state.temperature,
                    "max_tokens": st.session_state.max_tokens
                },
                headers=headers
            )
            result = handle_response(response, rag_data)
            if result:
                manage_chat_history(model_type, "assistant", result)
            return result

        elif model_type == "æ™ºè°±æ¸…è¨€":
            api_key = st.session_state.api_keys.get("æ™ºè°±æ¸…è¨€", "")
            if not api_key:
                st.error("è¯·æä¾› æ™ºè°±æ¸…è¨€ API å¯†é’¥ï¼")
                return None
            headers["Authorization"] = f"Bearer {api_key}"
            
            response = requests.post(
                "https://open.bigmodel.cn/api/paas/v4/chat/completions",
                json={
                    "model": "glm-4",
                    "messages": messages,
                    "temperature": st.session_state.temperature,
                    "max_tokens": st.session_state.max_tokens
                },
                headers=headers
            )
            result = handle_response(response, rag_data)
            if result:
                manage_chat_history(model_type, "assistant", result)
            return result

        elif model_type == "MiniMax":
            api_key = st.session_state.api_keys.get("MiniMax", "")
            if not api_key:
                st.error("è¯·æä¾› MiniMax API å¯†é’¥ï¼")
                return None
            headers["Authorization"] = f"Bearer {api_key}"
            
            response = requests.post(
                "https://api.minimax.chat/v1/text/chatcompletion_v2",
                json={
                    "model": "abab5.5-chat",
                    "messages": messages,
                    "temperature": st.session_state.temperature,
                    "max_tokens": st.session_state.max_tokens
                },
                headers=headers
            )
            result = handle_response(response, rag_data)
            if result:
                manage_chat_history(model_type, "assistant", result)
            return result

        elif model_type == "DALL-E(æ–‡ç”Ÿå›¾)":
            api_key = st.session_state.api_keys.get("OpenAI", "")
            if not api_key:
                st.error("è¯·æä¾› DALL-E(æ–‡ç”Ÿå›¾) API å¯†é’¥ï¼")
                return None
            headers["Authorization"] = f"Bearer {st.session_state.api_keys['OpenAI']}"
            
            response = requests.post(
                "https://api.openai.com/v1/images/generations",
                json={
                    "prompt": prompt,
                    "n": 1,
                    "size": "512x512"
                },
                headers=headers
            )
            response_json = response.json()
            if "data" in response_json and len(response_json["data"]) > 0:
                image_url = response_json["data"][0]["url"]
                return image_url
            else:
                st.error(f"DALL-E API è¿”å›æ ¼å¼å¼‚å¸¸: {response_json}")
                return None

        elif model_type == "DeepSeek-R1(æ·±åº¦æ¨ç†)":
            api_key = st.session_state.api_keys.get("DeepSeek", "")
            if not api_key:
                st.error("è¯·æä¾› DeepSeek API å¯†é’¥ï¼")
                return None
            headers["Authorization"] = f"Bearer {api_key}"
            
            # æ„å»ºå¢å¼ºçš„æç¤ºè¯
            enhanced_prompt = prompt
            if st.session_state.selected_assistant:
                domain = next(k for k, v in st.session_state.assistant_market.items() 
                            if st.session_state.selected_assistant in v)
                role_prompt = st.session_state.assistant_market[domain][st.session_state.selected_assistant]
                enhanced_prompt = f"{role_prompt}\n\nè¯·ä»¥{st.session_state.selected_assistant}çš„èº«ä»½å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š\n{prompt}"
            
            # è·å–å†å²æ¶ˆæ¯
            history = get_chat_history(model_type)
            if history:
                # å°†æœ€è¿‘çš„å¯¹è¯å†å²æ·»åŠ åˆ°æç¤ºè¯ä¸­
                recent_history = history[-6:]  # ä¿ç•™æœ€è¿‘3è½®å¯¹è¯
                history_text = "\n".join([f"{'ç”¨æˆ·' if msg['role']=='user' else 'åŠ©æ‰‹'}: {msg['content']}" 
                                        for msg in recent_history])
                enhanced_prompt = f"ä»¥ä¸‹æ˜¯å†å²å¯¹è¯ï¼š\n{history_text}\n\nå½“å‰é—®é¢˜ï¼š{enhanced_prompt}"
            
            response = requests.post(
                "https://api.deepseek.com/v1/chat/completions",
                json={
                    "model": "deepseek-reasoner",
                    "messages": [{"role": "user", "content": enhanced_prompt}],
                    "temperature": st.session_state.temperature,
                    "max_tokens": st.session_state.max_tokens
                },
                headers=headers
            )
            result = handle_response(response, rag_data)
            if result:
                manage_chat_history(model_type, "assistant", result)
            return result

        elif model_type == "o1(æ·±åº¦æ¨ç†)":
            api_key = st.session_state.api_keys.get("OpenAI", "")
            if not api_key:
                st.error("è¯·æä¾› o1 API å¯†é’¥ï¼")
                return None
            headers["Authorization"] = f"Bearer {api_key}"
            
            # æ„å»ºå¢å¼ºçš„æç¤ºè¯
            enhanced_prompt = prompt
            if st.session_state.selected_assistant:
                domain = next(k for k, v in st.session_state.assistant_market.items() 
                            if st.session_state.selected_assistant in v)
                role_prompt = st.session_state.assistant_market[domain][st.session_state.selected_assistant]
                enhanced_prompt = f"{role_prompt}\n\nè¯·ä»¥{st.session_state.selected_assistant}çš„èº«ä»½å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š\n{prompt}"
            
            # è·å–å†å²æ¶ˆæ¯
            history = get_chat_history(model_type)
            if history:
                # å°†æœ€è¿‘çš„å¯¹è¯å†å²æ·»åŠ åˆ°æç¤ºè¯ä¸­
                recent_history = history[-6:]  # ä¿ç•™æœ€è¿‘3è½®å¯¹è¯
                history_text = "\n".join([f"{'ç”¨æˆ·' if msg['role']=='user' else 'åŠ©æ‰‹'}: {msg['content']}" 
                                        for msg in recent_history])
                enhanced_prompt = f"ä»¥ä¸‹æ˜¯å†å²å¯¹è¯ï¼š\n{history_text}\n\nå½“å‰é—®é¢˜ï¼š{enhanced_prompt}"
            
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                json={
                    "model": "o1-mini",
                    "messages": [{"role": "user", "content": enhanced_prompt}],
                    "max_completion_tokens": st.session_state.max_tokens
                },
                headers=headers
            )
            result = handle_response(response, rag_data)
            if result:
                manage_chat_history(model_type, "assistant", result)
            return result

        elif model_type == "Kimi(è§†è§‰ç†è§£)":
            api_key = st.session_state.api_keys.get("Kimi(è§†è§‰ç†è§£)", "")
            if not api_key:
                st.error("è¯·æä¾› Kimi(è§†è§‰ç†è§£) API å¯†é’¥ï¼")
                return None
            headers["Authorization"] = f"Bearer {api_key}"
            
            response = requests.post(
                "https://api.moonshot.cn/v1/chat/completions",
                json={
                    "model": "moonshot-v1-8k-vision-preview",
                    "messages": messages
                },
                headers=headers
            )
            result = handle_response(response)
            if result:
                manage_chat_history(model_type, "assistant", result)
            return result

        elif model_type == "GPTs(èŠå¤©ã€è¯­éŸ³è¯†åˆ«)":
            api_key = st.session_state.api_keys.get("OpenAI", "")
            if not api_key:
                st.error("è¯·æä¾› OpenAI API å¯†é’¥ï¼")
                return None
            headers["Authorization"] = f"Bearer {api_key}"
            
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                json={
                    "model": "gpt-4o",
                    "messages": messages,
                    "temperature": st.session_state.temperature,
                    "max_tokens": st.session_state.max_tokens
                },
                headers=headers
            )
            result = handle_response(response, rag_data)
            if result:
                manage_chat_history(model_type, "assistant", result)
            return result

        elif model_type == "grok2":
            api_key = st.session_state.api_keys.get("xAI", "")
            if not api_key:
                st.error("è¯·æä¾› xAI API å¯†é’¥ï¼")
                return None
            headers["Authorization"] = f"Bearer {api_key}"
            
            response = requests.post(
                "https://api.x.ai/v1/chat/completions",
                json={
                    "model": "grok-2-latest",
                    "messages": messages,
                    "temperature": st.session_state.temperature,
                    "max_tokens": st.session_state.max_tokens
                },
                headers=headers
            )
            result = handle_response(response, rag_data)
            if result:
                manage_chat_history(model_type, "assistant", result)
            return result

        elif model_type == "æ··å…ƒç”Ÿæ–‡":
            api_key = st.session_state.api_keys.get("æ··å…ƒç”Ÿæ–‡", "")
            if not api_key:
                st.error("è¯·æä¾›è…¾è®¯æ··å…ƒ API å¯†é’¥ï¼")
                return None
            
            try:
                client = OpenAI(
                    api_key=api_key,
                    base_url="https://api.hunyuan.cloud.tencent.com/v1"
                )
                
                response = client.chat.completions.create(
                    model="hunyuan-turbo",
                    messages=messages,
                    temperature=st.session_state.temperature,
                    max_tokens=st.session_state.max_tokens
                )
                
                if response.choices:
                    result = response.choices[0].message.content
                    manage_chat_history(model_type, "assistant", result)
                    return result
                else:
                    st.error("API è¿”å›æ ¼å¼å¼‚å¸¸")
                    return None
            
            except Exception as e:
                st.error(f"è°ƒç”¨æ··å…ƒæ¨¡å‹æ—¶å‡ºé”™ï¼š{str(e)}")
                return None

        else:
            # é»˜è®¤è°ƒç”¨ä½¿ç”¨ RAG ç”Ÿæˆç­”æ¡ˆ
            result = rag_generate_response(prompt)
            if result:
                manage_chat_history(model_type, "assistant", result)
            return result

    except Exception as e:
        st.error(f"APIè°ƒç”¨å¤±è´¥: {str(e)}")
        return None

def handle_response(response, rag_data=None):
    """å¤„ç† API å“åº”"""
    try:
        if response.status_code == 200:
            response_json = response.json()
            if "choices" in response_json and len(response_json["choices"]) > 0:
                answer = response_json["choices"][0]["message"]["content"]
            elif "result" in response_json:
                # é’ˆå¯¹æ–‡å¿ƒä¸€è¨€è¿”å›æ ¼å¼å¤„ç†
                answer = response_json["result"]
            elif "data" in response_json and isinstance(response_json["data"], list) and len(response_json["data"]) > 0:
                # é’ˆå¯¹ DALL-E è¿”å›æ ¼å¼å¤„ç†
                if "url" in response_json["data"][0]:
                    answer = response_json["data"][0]["url"]
                else:
                    st.error(f"API è¿”å›æ ¼å¼å¼‚å¸¸: {response_json}")
                    return None
            else:
                st.error(f"API è¿”å›æ ¼å¼å¼‚å¸¸: {response_json}")
                return None

            if rag_data and isinstance(answer, str):  # ç¡®ä¿æ˜¯æ–‡æœ¬æ‰æ·»åŠ å¼•ç”¨
                answer += "\n\nå¼•ç”¨æ¥æºï¼š\n" + "\n".join([f"- {source}" for source in rag_data])
            return answer
        elif response.status_code == 503:
            st.error("æœåŠ¡å™¨ç¹å¿™ï¼Œè¯·ç¨åå†è¯•ã€‚")
            return None
        else:
            st.error(f"API è¯·æ±‚å¤±è´¥ï¼Œé”™è¯¯ç ï¼š{response.status_code}")
            return None
    except ValueError as e:
        st.error(f"å“åº”è§£æå¤±è´¥: {str(e)}")
        return None

# ä½¿ç”¨ langchain å®ç° RAGï¼šåŠ è½½æ–‡æ¡£ã€åˆ†å‰²ã€åµŒå…¥ã€ç´¢å¼•
def rag_index_document(content, source):
    """å°†æ–‡æ¡£æ·»åŠ åˆ°å‘é‡æ•°æ®åº“"""
    try:
        # æ£€æŸ¥å­˜å‚¨è·¯å¾„
        if not st.session_state.get("chromadb_path"):
            st.error("âš ï¸ è¯·å…ˆåœ¨ RAGçŸ¥è¯†åº“è®¾ç½®ä¸ç®¡ç† ä¸­è®¾ç½®å­˜å‚¨è·¯å¾„ï¼")
            return False
            
        # æ£€æŸ¥å†…å®¹
        if not content or not isinstance(content, str):
            st.error("âš ï¸ æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®")
            return False
            
        # æ¸…ç†æ–‡æœ¬å†…å®¹
        content = clean_text(content)
        if not content:
            st.error("âš ï¸ æ¸…ç†åçš„æ–‡æœ¬å†…å®¹ä¸ºç©º")
            return False
            
        # æ–‡æœ¬åˆ†å‰²
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            length_function=len,
        )
        texts = text_splitter.split_text(content)
        
        if not texts:
            st.error("âš ï¸ æ–‡æœ¬åˆ†å‰²åä¸ºç©º")
            return False
            
        # é™åˆ¶æ–‡æœ¬å—æ•°é‡
        max_chunks = 100
        if len(texts) > max_chunks:
            st.warning(f"æ–‡æ¡£è¿‡å¤§ï¼Œå°†åªå¤„ç†å‰ {max_chunks} ä¸ªæ–‡æœ¬å—")
            texts = texts[:max_chunks]
            
        # è·å–å‘é‡åº“å®ä¾‹
        vectorstore = get_vector_store()
        if not vectorstore:
            return False
        
        try:
            # åˆ†æ‰¹æ·»åŠ æ–‡æ¡£
            batch_size = 20
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                batch_metadatas = [{"source": source} for _ in batch_texts]
                vectorstore.add_texts(
                    texts=batch_texts,
                    metadatas=batch_metadatas
                )
            
            # ä¿å­˜å‘é‡åº“
            save_path = os.path.join(st.session_state.chromadb_path, "faiss_index")
            vectorstore.save_local(save_path)
            
            # æ›´æ–°ä¼šè¯çŠ¶æ€
            st.session_state.vector_store = vectorstore
            if source not in st.session_state.rag_data:
                st.session_state.rag_data.append(source)
            st.success(f"âœ… æˆåŠŸæ·»åŠ  {len(texts)} ä¸ªæ–‡æœ¬å—åˆ°çŸ¥è¯†åº“")
            return True
            
        except Exception as e:
            st.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥ï¼š{str(e)}")
            return False
            
    except Exception as e:
        st.error(f"âŒ å¤„ç†æ–‡æ¡£å¤±è´¥ï¼š{str(e)}")
        import traceback
        st.error(f"è¯¦ç»†é”™è¯¯ï¼š{traceback.format_exc()}")
        return False

def rag_generate_response(query):
    """ç”Ÿæˆ RAG å“åº”"""
    try:
        # è·å–å‘é‡åº“å®ä¾‹
        vectorstore = get_vector_store()
        if not vectorstore:
            return "è¯·å…ˆä¸Šä¼ æ–‡ä»¶æˆ–ç½‘å€åˆ°çŸ¥è¯†åº“ã€‚"
        
        # é™åˆ¶ç›¸ä¼¼æ€§æœç´¢çš„æ•°é‡
        k = 2  # å‡å°‘è¿”å›çš„æ–‡æ¡£æ•°é‡
        
        try:
            # æ‰§è¡Œç›¸ä¼¼æ€§æœç´¢
            docs = vectorstore.similarity_search(query, k=k)
            
            if not docs:
                return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•è°ƒæ•´é—®é¢˜æˆ–æ·»åŠ æ›´å¤šç›¸å…³æ–‡æ¡£ã€‚"
            
            # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
            max_context_length = 2000
            context = "\n\n".join([doc.page_content[:max_context_length] for doc in docs])
            sources = "\n".join([f"- {doc.metadata.get('source', 'æœªçŸ¥æ¥æº')}" for doc in docs])
            
            # æ„å»ºæç¤ºè¯
            prompt = f"""åŸºäºä»¥ä¸‹å‚è€ƒä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœå‚è€ƒä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚

å‚è€ƒä¿¡æ¯ï¼š
{context}

é—®é¢˜ï¼š{query}

è¯·æä¾›å‡†ç¡®ã€ç›¸å…³çš„å›ç­”ã€‚
"""
            # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›ç­”
            response = call_model_api(prompt, st.session_state.selected_model)
            if response:
                return f"{response}\n\næ¥æºï¼š\n{sources}"
            return "ç”Ÿæˆå›ç­”å¤±è´¥ï¼Œè¯·é‡è¯•ã€‚"
            
        except Exception as e:
            st.error(f"æœç´¢ç›¸å…³æ–‡æ¡£å¤±è´¥ï¼š{str(e)}")
            return "å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™ï¼Œè¯·é‡è¯•ã€‚"
            
    except Exception as e:
        st.error(f"âŒ ç”Ÿæˆå›ç­”å¤±è´¥ï¼š{str(e)}")
        import traceback
        st.error(f"è¯¦ç»†é”™è¯¯ï¼š{traceback.format_exc()}")
        return None

def handle_file_upload(uploaded_files):
    """å¤„ç†ä¸Šä¼ æ–‡ä»¶ï¼Œæ ¹æ® RAG çŠ¶æ€åŠæ–‡ä»¶ç±»å‹æ‰§è¡Œä¸åŒæ“ä½œï¼š
       - RAG æ¨¡å¼ä¸‹ï¼šæ–‡æœ¬ã€è¡¨æ ¼ç±»æ–‡ä»¶åŠ å…¥çŸ¥è¯†åº“ï¼›
       - é RAG æ¨¡å¼ä¸‹ï¼š
           å›¾ç‰‡æ–‡ä»¶ -> è§†è§‰åˆ†æ
           è¯­éŸ³æ–‡ä»¶ -> è¯­éŸ³è¯†åˆ«
           æ–‡æœ¬æ–‡ä»¶ -> æ–‡æœ¬æ€»ç»“
    """
    if not uploaded_files:
        return

    if not isinstance(uploaded_files, list):
        uploaded_files = [uploaded_files]

    for uploaded_file in uploaded_files:
        if not hasattr(uploaded_file, "name"):
            st.error("ä¸Šä¼ çš„æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘åç§°å±æ€§ã€‚")
            continue

        file_name = uploaded_file.name
        file_type = uploaded_file.type.split("/")[-1].lower()
        try:
            if st.session_state.rag_enabled:
                # RAG æ¨¡å¼ä¸‹ï¼Œä»…å¤„ç†æ–‡æœ¬ã€è¡¨æ ¼ç±»æ–‡ä»¶åŠ å…¥çŸ¥è¯†åº“
                if file_type in ["txt", "pdf", "docx", "doc", "csv", "xlsx", "xls"]:
                    content = extract_text_from_file(uploaded_file)
                    if content:
                        if rag_index_document(content, file_name):
                            st.session_state.rag_data.append(file_name)
                            st.success(f"æ–‡ä»¶ {file_name} å·²æˆåŠŸåŠ å…¥ RAG çŸ¥è¯†åº“")
                else:
                    st.warning(f"RAG æ¨¡å¼ä¸‹ï¼Œæ–‡ä»¶ {file_name} çš„ç±»å‹ï¼ˆ{file_type}ï¼‰ä¸æ”¯æŒåŠ å…¥çŸ¥è¯†åº“ã€‚")
            else:
                # é RAG æ¨¡å¼ï¼Œæ ¹æ®æ–‡ä»¶ç±»å‹è°ƒç”¨å¯¹åº”åŠŸèƒ½
                if file_type in ["jpg", "jpeg", "png"]:
                    with st.spinner("ğŸ–¼ï¸ æ­£åœ¨åˆ†æå›¾ç‰‡..."):
                        if st.session_state.selected_model == "Kimi(è§†è§‰ç†è§£)":  # åªä½¿ç”¨ Kimi è¿›è¡Œè§†è§‰ç†è§£
                            if "Kimi(è§†è§‰ç†è§£)" not in st.session_state.api_keys:
                                st.error("è¯·å…ˆé…ç½® Kimi(è§†è§‰ç†è§£) API å¯†é’¥")
                            else:
                                image_content = uploaded_file.getvalue()
                                encoded_image = base64.b64encode(image_content).decode('utf-8')
                                
                                headers = {
                                    "Content-Type": "application/json",
                                    "Authorization": f"Bearer {st.session_state.api_keys['Kimi(è§†è§‰ç†è§£)']}"
                                }
                                
                                payload = {
                                    "model": "moonshot-v1-8k-vision-preview",
                                    "messages": [
                                        {
                                            "role": "user",
                                            "content": [
                                                {
                                                    "type": "text",
                                                    "text": "è¯·è¯¦ç»†åˆ†æè¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦å¯¹è±¡ã€åœºæ™¯ã€ç»†èŠ‚ç­‰æ–¹é¢ã€‚"
                                                },
                                                {
                                                    "type": "image_url",
                                                    "image_url": {
                                                        "url": f"data:image/jpeg;base64,{encoded_image}"
                                                    }
                                                }
                                            ]
                                        }
                                    ]
                                }
                                
                                response = requests.post(
                                    "https://api.moonshot.cn/v1/chat/completions",
                                    json=payload,
                                    headers=headers
                                )
                                
                                if response.status_code == 200:
                                    result = response.json()["choices"][0]["message"]["content"]
                                    st.success("âœ… å›¾ç‰‡åˆ†æå®Œæˆ")
                                    with st.chat_message("assistant"):
                                        st.markdown(f"**å›¾ç‰‡åˆ†æç»“æœï¼š**\n\n{result}")
                                    st.session_state.messages.append({
                                        "role": "assistant",
                                        "content": f"å›¾ç‰‡ {uploaded_file.name} çš„åˆ†æç»“æœï¼š\n\n{result}",
                                        "type": "text"
                                    })
                                else:
                                    st.error(f"âŒ å›¾ç‰‡åˆ†æå¤±è´¥ï¼š{response.text}")
                elif file_type in ["mp3", "wav", "m4a", "mpeg"]:
                    st.write(f"æ­£åœ¨è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼š{file_name}")
                    speech_result = perform_speech_recognition(uploaded_file.getvalue())
                    if speech_result:
                        st.write("è¯­éŸ³è¯†åˆ«ç»“æœï¼š")
                        st.write(speech_result)
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": f"è¯­éŸ³è¯†åˆ«ç»“æœï¼š\n{speech_result}",
                            "type": "text"
                        })
                elif file_type in ["txt", "pdf", "docx", "doc"]:
                    content = extract_text_from_file(uploaded_file)
                    if content:
                        st.write(f"æ­£åœ¨æ€»ç»“æ–‡æœ¬ï¼š{file_name}")
                        summary_result = perform_text_summary(content)
                        if summary_result:
                            st.write("æ–‡æœ¬æ€»ç»“ç»“æœï¼š")
                            st.write(summary_result)
                            st.session_state.messages.append({
                                "role": "assistant",
                                "content": f"æ–‡æœ¬æ€»ç»“ç»“æœï¼š\n{summary_result}",
                                "type": "text"
                            })
                else:
                    st.warning(f"æ–‡ä»¶ {file_name} çš„ç±»å‹ï¼ˆ{file_type}ï¼‰ä¸æ”¯æŒå¤„ç†ã€‚")
        except Exception as e:
            st.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥ ({file_name}): {str(e)}")

def extract_text_from_file(file):
    """ä»ä¸åŒç±»å‹çš„æ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹"""
    try:
        file_type = file.name.split('.')[-1].lower()
        content = file.read()
        
        if file_type == 'txt':
            # å¤„ç†æ–‡æœ¬æ–‡ä»¶
            return content.decode('utf-8')
        elif file_type == 'pdf':
            # å¤„ç† PDF æ–‡ä»¶
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(content))
            return "\n".join([page.extract_text() for page in pdf_reader.pages])
        elif file_type in ['docx', 'doc']:
            # å¤„ç† Word æ–‡ä»¶
            doc = Document(io.BytesIO(content))
            return "\n".join([para.text for para in doc.paragraphs])
        elif file_type in ['csv', 'xlsx', 'xls']:
            # å¤„ç†è¡¨æ ¼æ–‡ä»¶
            if file_type == 'csv':
                df = pd.read_csv(io.BytesIO(content))
            else:
                df = pd.read_excel(io.BytesIO(content))
            return df.to_string()
        else:
            st.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼š{file_type}")
            return None
    except Exception as e:
        st.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")
        return None

def perform_speech_recognition(audio_bytes):
    """
    ä½¿ç”¨å½“å‰é€‰æ‹©çš„æ¨¡å‹è¿›è¡Œè¯­éŸ³è¯†åˆ«
    """
    api_key = st.session_state.api_keys.get("OpenAI", "")
    if not api_key:
        st.error("è¯·æä¾› OpenAI API å¯†é’¥ä»¥è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼")
        return None
    
    try:
        # åˆ›å»º OpenAI å®¢æˆ·ç«¯
        client = OpenAI(api_key=api_key)
        
        # å°†éŸ³é¢‘æ•°æ®è½¬æ¢ä¸ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_file:
            temp_file.write(audio_bytes)
            temp_file_path = temp_file.name
        
        with open(temp_file_path, 'rb') as audio_file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file
            )
        
        os.unlink(temp_file_path)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        return transcript.text
        
    except Exception as e:
        st.error(f"è¯­éŸ³è¯†åˆ«å¤±è´¥ï¼š{str(e)}")
        return None

def perform_text_summary(text):
    """
    ä½¿ç”¨å½“å‰é€‰æ‹©çš„æ¨¡å‹å¯¹æ–‡æœ¬è¿›è¡Œæ€»ç»“
    """
    try:
        summary_prompt = f"è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œç®€æ˜æ‰¼è¦çš„æ€»ç»“ï¼š\n\n{text}"
        response = call_model_api(summary_prompt, st.session_state.selected_model)
        return response
    except Exception as e:
        st.error(f"æ–‡æœ¬æ€»ç»“å¤±è´¥ï¼š{str(e)}")
        return None

def retrieve_relevant_content(query):
    """
    åˆ©ç”¨ langchain å°è£…çš„å‘é‡åº“æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£ï¼Œ
    è¿”å›åŒ…å«æ¥æºä¿¡æ¯çš„åˆ—è¡¨ã€‚
    """
    vectorstore = get_vector_store()
    try:
        results = vectorstore.similarity_search(query, k=3)
    except Exception as e:
        st.error(f"æ£€ç´¢æ—¶å‡ºç°é”™è¯¯: {str(e)}")
        return []
    # æå–æ–‡æ¡£ metadata ä¸­çš„ "source" ä¿¡æ¯ï¼›å¦‚æœä¸å­˜åœ¨åˆ™è¿”å› "æœªçŸ¥æ¥æº"
    return [doc.metadata.get("source", "æœªçŸ¥æ¥æº") for doc in results]

def fetch_url_content(url):
    """è·å–ç½‘é¡µå†…å®¹å¹¶æå–æœ‰æ•ˆæ–‡æœ¬"""
    try:
        # æ·»åŠ è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, verify=False, timeout=10)
        response.raise_for_status()
        
        # ä½¿ç”¨ BeautifulSoup æå–æ–‡æœ¬å†…å®¹
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼å…ƒç´ 
        for script in soup(["script", "style"]):
            script.decompose()
        
        # è·å–æ–‡æœ¬å¹¶å¤„ç†
        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        return text
    except Exception as e:
        st.error(f"è·å–ç½‘é¡µå†…å®¹å¤±è´¥ï¼š{str(e)}")
        return None

def clear_vector_store():
    """æ¸…ç©ºå‘é‡æ•°æ®åº“"""
    try:
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not st.session_state.get("chromadb_path"):
            st.error("æœªè®¾ç½®å‘é‡åº“å­˜å‚¨è·¯å¾„")
            return False
            
        db_path = st.session_state.chromadb_path
        if os.path.exists(db_path):
            import shutil
            shutil.rmtree(db_path)
            # é‡æ–°åˆ›å»ºç›®å½•
            os.makedirs(db_path, exist_ok=True)
            
        st.session_state.vector_store = None
        st.session_state.rag_data = []
        st.success("âœ… çŸ¥è¯†åº“å·²æ¸…ç©º")
        st.rerun()
        return True
    except Exception as e:
        st.error(f"æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥ï¼š{str(e)}")
        return False

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬å†…å®¹"""
    if not text:
        return ""
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text).strip()
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’ŒåŸºæœ¬æ ‡ç‚¹
    text = re.sub(r'[^\w\s\u4e00-\u9fff,.?!ï¼Œã€‚ï¼Ÿï¼:ï¼š;ï¼›""''()ï¼ˆï¼‰ã€Šã€‹<>]', '', text)
    return text

def is_financial_domain(url):
    """åˆ¤æ–­æ˜¯å¦ä¸ºè´¢ç»é‡‘èç›¸å…³çš„é«˜è´¨é‡åŸŸå"""
    try:
        domain = urlparse(url).netloc.lower()
        
        # è´¢ç»é‡‘èç½‘ç«™ä¼˜å…ˆçº§
        financial_domains = {
            # å®˜æ–¹æœºæ„
            'pbc.gov.cn': 10,     # ä¸­å›½äººæ°‘é“¶è¡Œ
            'csrc.gov.cn': 10,    # ä¸­å›½è¯ç›‘ä¼š
            'safe.gov.cn': 10,    # å¤–æ±‡ç®¡ç†å±€
            'stats.gov.cn': 10,   # å›½å®¶ç»Ÿè®¡å±€
            'mof.gov.cn': 10,     # è´¢æ”¿éƒ¨
            
            # äº¤æ˜“æ‰€
            'sse.com.cn': 9,      # ä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€
            'szse.cn': 9,         # æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€
            'cffex.com.cn': 9,    # ä¸­å›½é‡‘èæœŸè´§äº¤æ˜“æ‰€
            
            # é‡‘èé—¨æˆ·ç½‘ç«™
            'eastmoney.com': 8,   # ä¸œæ–¹è´¢å¯Œ
            'finance.sina.com.cn': 8,  # æ–°æµªè´¢ç»
            'caixin.com': 8,      # è´¢æ–°ç½‘
            'yicai.com': 8,       # ç¬¬ä¸€è´¢ç»
            'stcn.com': 8,        # è¯åˆ¸æ—¶æŠ¥ç½‘
            'cnstock.com': 8,     # ä¸­å›½è¯åˆ¸ç½‘
            '21jingji.com': 8,    # 21ä¸–çºªç»æµç½‘
            
            # è´¢ç»åª’ä½“
            'bloomberg.cn': 8,     # å½­åš
            'ftchinese.com': 8,   # FTä¸­æ–‡ç½‘
            'nbd.com.cn': 7,      # æ¯æ—¥ç»æµæ–°é—»
            'ce.cn': 7,           # ä¸­å›½ç»æµç½‘
            'jrj.com.cn': 7,      # é‡‘èç•Œ
            'hexun.com': 7,       # å’Œè®¯ç½‘
            
            # ç ”ç©¶æœºæ„
            'cfets.org.cn': 7,    # ä¸­å›½å¤–æ±‡äº¤æ˜“ä¸­å¿ƒ
            'chinabond.com.cn': 7, # ä¸­å›½å€ºåˆ¸ä¿¡æ¯ç½‘
            'shibor.org': 7,      # Shiborå®˜ç½‘
            
            # å›½é™…é‡‘èç½‘ç«™
            'reuters.com': 8,      # è·¯é€ç¤¾
            'bloomberg.com': 8,    # å½­åš
            'wsj.com': 8,         # åå°”è¡—æ—¥æŠ¥
            'ft.com': 8,          # é‡‘èæ—¶æŠ¥
            'economist.com': 8,    # ç»æµå­¦äºº
            
            # å…¶ä»–ç›¸å…³ç½‘ç«™
            'investing.com': 7,    # è‹±ä¸ºè´¢æƒ…
            'marketwatch.com': 7,  # å¸‚åœºè§‚å¯Ÿ
            'cnfol.com': 6,       # ä¸­é‡‘åœ¨çº¿
            'stockstar.com': 6,   # è¯åˆ¸ä¹‹æ˜Ÿ
            '10jqka.com.cn': 6,   # åŒèŠ±é¡ºè´¢ç»
        }
        
        # æ£€æŸ¥åŸŸåä¼˜å…ˆçº§
        for known_domain, priority in financial_domains.items():
            if known_domain in domain:
                return priority
                
        return 0  # éé‡‘èç½‘ç«™è¿”å›0ä¼˜å…ˆçº§
    except:
        return 0

def perform_web_search(query, max_results=10):
    """æ‰§è¡Œä¼˜åŒ–çš„è´¢ç»é‡‘èæœç´¢"""
    try:
        # ä¼˜åŒ–æœç´¢æŸ¥è¯¢
        financial_keywords = ['é‡‘è', 'è´¢ç»', 'ç»æµ', 'è‚¡å¸‚', 'åŸºé‡‘', 'å€ºåˆ¸', 'å¤–æ±‡', 
                            'æœŸè´§', 'ç†è´¢', 'æŠ•èµ„', 'è¯åˆ¸', 'é“¶è¡Œ', 'ä¿é™©', 'é‡‘ä»·']
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ è´¢ç»å…³é”®è¯
        if not any(keyword in query for keyword in financial_keywords):
            # æ·»åŠ è´¢ç»ç›¸å…³å…³é”®è¯ä»¥æé«˜ç›¸å…³æ€§
            optimized_query = query + ' è´¢ç»'
        else:
            optimized_query = query
        
        # ä½¿ç”¨ DuckDuckGoSearchRun è¿›è¡Œä¸»æœç´¢
        search_tool = DuckDuckGoSearchRun()
        initial_results = search_tool.run(optimized_query)
        
        # ä½¿ç”¨ DDGS è¿›è¡Œè¡¥å……æœç´¢
        with DDGS() as ddgs:
            detailed_results = list(ddgs.text(
                optimized_query,
                max_results=max_results,
                region='cn-zh',
                safesearch='moderate',
                timelimit='m'  # é™åˆ¶æœ€è¿‘ä¸€ä¸ªæœˆçš„ç»“æœï¼Œä¿è¯ä¿¡æ¯æ—¶æ•ˆæ€§
            ))
        
        # ç»“æœå¤„ç†å’Œæ’åº
        processed_results = []
        seen_content = set()
        
        if detailed_results:
            for result in detailed_results:
                title = clean_text(result.get('title', ''))
                snippet = clean_text(result.get('body', ''))
                link = result.get('link', '')
                
                # å†…å®¹å»é‡æ£€æŸ¥
                content_hash = f"{title}_{snippet}"
                if content_hash in seen_content:
                    continue
                seen_content.add(content_hash)
                
                # è®¡ç®—åŸŸåè´¨é‡åˆ†æ•°
                domain_score = is_financial_domain(link)
                
                # è®¡ç®—å†…å®¹ç›¸å…³æ€§åˆ†æ•°
                relevance_score = sum(1 for word in query.lower().split() 
                                    if word in title.lower() or word in snippet.lower())
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«è´¢ç»å…³é”®è¯
                financial_relevance = sum(1 for keyword in financial_keywords 
                                        if keyword in title or keyword in snippet)
                
                # ç»¼åˆè¯„åˆ†
                total_score = domain_score * 3 + relevance_score * 2 + financial_relevance * 2
                
                if domain_score > 0 or financial_relevance > 0:  # åªä¿ç•™é‡‘èç›¸å…³ç½‘ç«™çš„å†…å®¹
                    processed_results.append({
                        'title': title,
                        'snippet': snippet,
                        'link': link,
                        'score': total_score
                    })
        
        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        processed_results.sort(key=lambda x: x['score'], reverse=True)
        
        # æ„å»ºæœ€ç»ˆå“åº”
        final_response = "è´¢ç»ç›¸å…³æœç´¢ç»“æœï¼š\n\n"
        
        # æ·»åŠ åˆæ­¥æœç´¢ç»“æœ
        if initial_results and any(keyword in initial_results.lower() for keyword in financial_keywords):
            final_response += f"{initial_results}\n\n"
        
        # æ·»åŠ é«˜è´¨é‡è¡¥å……ç»“æœ
        if processed_results:
            final_response += "è¡¥å……ä¿¡æ¯ï¼š\n"
            for idx, result in enumerate(processed_results[:5], 1):
                if result['score'] > 4:  # æé«˜æ˜¾ç¤ºé˜ˆå€¼ï¼Œç¡®ä¿é«˜è´¨é‡ç»“æœ
                    final_response += f"{idx}. **{result['title']}**\n"
                    final_response += f"   {result['snippet']}\n"
                    final_response += f"   æ¥æºï¼š[{urlparse(result['link']).netloc}]({result['link']})\n\n"
        
        return final_response.strip()
    
    except Exception as e:
        st.error(f"è´¢ç»ä¿¡æ¯æœç´¢å¤±è´¥: {str(e)}")
        return None

def get_search_response(query):
    """ç”Ÿæˆä¼˜åŒ–çš„è´¢ç»æœç´¢å“åº”ï¼Œå¹¶ç”±å¤§æ¨¡å‹æ€»ç»“"""
    try:
        # è·å–æœç´¢ç»“æœ
        search_results = perform_web_search(query)
        if not search_results:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„è´¢ç»ä¿¡æ¯ã€‚"
        
        # æ„å»ºæç¤ºè¯ï¼Œè®©å¤§æ¨¡å‹è¿›è¡Œæ€»ç»“
        summary_prompt = f"""
è¯·é’ˆå¯¹ä»¥ä¸‹ç”¨æˆ·é—®é¢˜å’Œæœç´¢ç»“æœï¼Œè¿›è¡Œä¸“ä¸šçš„æ€»ç»“åˆ†æï¼š

ç”¨æˆ·é—®é¢˜ï¼š{query}

æœç´¢ç»“æœï¼š
{search_results}

è¯·ä½ ä½œä¸ºé‡‘èä¸“å®¶ï¼š
1. æå–è¦ç‚¹ï¼Œç›´æ¥å›ç­”ç”¨æˆ·çš„æ ¸å¿ƒé—®é¢˜
2. ç¡®ä¿ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œæ—¶æ•ˆæ€§
3. å¦‚æœ‰å¿…è¦ï¼Œç»™å‡ºä¸“ä¸šçš„å»ºè®®æˆ–é£é™©æç¤º
4. ä¿æŒç®€æ´æ¸…æ™°ï¼Œçªå‡ºé‡ç‚¹

è¯·ä»¥ä¸“ä¸šã€å®¢è§‚çš„å£å»å›ç­”ã€‚
"""
        # è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œæ€»ç»“
        summary = call_model_api(summary_prompt, st.session_state.selected_model)
        
        # æ„å»ºæœ€ç»ˆå“åº”
        response = "ğŸ“Š **æ ¸å¿ƒå›ç­”ï¼š**\n\n"
        response += f"{summary}\n\n"
        response += "---\n"
        response += "ğŸ” **è¯¦ç»†æœç´¢ç»“æœï¼š**\n\n"
        response += f"{search_results}\n\n"
        response += "---\n"
        response += "*ä»¥ä¸Šä¿¡æ¯æ¥è‡ªæƒå¨è´¢ç»é‡‘èç½‘ç«™ï¼Œå¹¶ç»AIåˆ†ææ•´ç†ã€‚è¯·æ³¨æ„ä¿¡æ¯æ—¶æ•ˆæ€§ï¼Œå»ºè®®è¿›ä¸€æ­¥æ ¸å®å…·ä½“æ•°æ®ã€‚*"
        
        return response

    except Exception as e:
        st.error(f"ç”Ÿæˆå›ç­”å¤±è´¥ï¼š{str(e)}")
        return None

def process_urls(urls_input):
    """å¤„ç†è¾“å…¥çš„ç½‘å€ï¼Œæå–å†…å®¹å¹¶æ·»åŠ åˆ° RAG çŸ¥è¯†åº“"""
    urls = [url.strip() for url in urls_input.split('\n') if url.strip()]
    
    for url in urls:
        with st.spinner(f"æ­£åœ¨å¤„ç†ç½‘å€ï¼š{url}"):
            try:
                # å‘é€ HTTP è¯·æ±‚è·å–ç½‘é¡µå†…å®¹
                response = requests.get(url, timeout=10)
                response.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
                
                # ä½¿ç”¨ BeautifulSoup è§£æç½‘é¡µå†…å®¹
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼å…ƒç´ 
                for script in soup(["script", "style"]):
                    script.decompose()
                
                # æå–æ–‡æœ¬å†…å®¹
                text = soup.get_text()
                
                # æ¸…ç†æ–‡æœ¬ï¼ˆç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦ï¼‰
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = ' '.join(chunk for chunk in chunks if chunk)
                
                if text:
                    # å°†ç½‘é¡µå†…å®¹æ·»åŠ åˆ° RAG çŸ¥è¯†åº“
                    if rag_index_document(text, url):
                        st.session_state.rag_data.append(url)
                        st.success(f"âœ… ç½‘å€ {url} å·²æˆåŠŸåŠ å…¥çŸ¥è¯†åº“")
                else:
                    st.warning(f"âš ï¸ ç½‘å€ {url} æœªæå–åˆ°æœ‰æ•ˆå†…å®¹")
                    
            except requests.RequestException as e:
                st.error(f"âŒ è®¿é—®ç½‘å€ {url} å¤±è´¥ï¼š{str(e)}")
            except Exception as e:
                st.error(f"âŒ å¤„ç†ç½‘å€ {url} æ—¶å‡ºé”™ï¼š{str(e)}")

def get_embeddings():
    """è·å– embeddings å®ä¾‹"""
    try:
        # ä½¿ç”¨ä¸­æ–‡åŸºç¡€æ¨¡å‹
        embeddings = HuggingFaceEmbeddings(
            model_name="shibing624/text2vec-base-chinese",
            cache_folder="models"
        )
        return embeddings
    except Exception as e:
        st.error(f"åˆå§‹åŒ– embeddings å¤±è´¥ï¼š{str(e)}")
        import traceback
        st.error(f"è¯¦ç»†é”™è¯¯ï¼š{traceback.format_exc()}")
        return None

# ====================
# ä¾§è¾¹æ é…ç½®
# ====================
with st.sidebar:
    st.header("âš™ï¸ ç³»ç»Ÿè®¾ç½®")

    # API å¯†é’¥ç®¡ç†
    st.subheader("APIå¯†é’¥ç®¡ç†")
    api_key_input = st.text_input(
        "è¾“å…¥ API å¯†é’¥",
        help="è¾“å…¥ä¸€ä¸ªAPIå¯†é’¥ï¼Œç”¨äºè®¿é—®æ‰€é€‰æ¨¡å‹",
        type="password"
    )
    api_keys_to_set = {
        "è±†åŒ…": api_key_input,
        "Kimi(è§†è§‰ç†è§£)": api_key_input,
        "DeepSeek": api_key_input,
        "é€šä¹‰åƒé—®": api_key_input,
        "æ··å…ƒç”Ÿæ–‡": api_key_input,
        "æ–‡å¿ƒä¸€è¨€": api_key_input,
        "æ™ºè°±æ¸…è¨€": api_key_input,
        "MiniMax": api_key_input,
        "OpenAI": api_key_input,
        "xAI": api_key_input
    }
    if api_key_input:
        for key, value in api_keys_to_set.items():
            st.session_state.api_keys[key] = value
        st.success("API å¯†é’¥å·²ä¿å­˜ï¼")

    # æ¨¡å‹é€‰æ‹©
    model_options = {
        "è±†åŒ…": ["ep-20250128163906-p4tb5"],
        "DeepSeek-V3": ["deepseek-chat"],
        "é€šä¹‰åƒé—®": ["qwen-plus"],
        "æ··å…ƒç”Ÿæ–‡": ["hunyuan-turbo"],
        "æ–‡å¿ƒä¸€è¨€": ["ERNIE-Bot"],
        "æ™ºè°±æ¸…è¨€": ["glm-4"],
        "MiniMax": ["abab5.5-chat"],
        "DALL-E(æ–‡ç”Ÿå›¾)": ["dall-e-3"],
        "DeepSeek-R1(æ·±åº¦æ¨ç†)": ["deepseek-reasoner"],
        "o1(æ·±åº¦æ¨ç†)": ["o1-mini"],
        "Kimi(è§†è§‰ç†è§£)": ["moonshot-v1-8k-vision-preview"],
        "GPTs(èŠå¤©ã€è¯­éŸ³è¯†åˆ«)": ["gpt-4"],
        "grok2": ["grok-2-latest"]
    }

    st.session_state.selected_model = st.selectbox(
        "é€‰æ‹©å¤§æ¨¡å‹",
        list(model_options.keys()),
        index=0
    )

    # åŠŸèƒ½é€‰æ‹©
    function_options = [
        "æ™ºèƒ½é—®ç­”",
        "æ–‡æœ¬ç¿»è¯‘",
        "æ–‡æœ¬æ€»ç»“",
        "æ–‡ç”Ÿå›¾",
        "æ·±åº¦æ¨ç†",
        "è§†è§‰ç†è§£",
        "è¯­éŸ³è¯†åˆ«"
    ]
    st.session_state.selected_function = st.selectbox(
        "é€‰æ‹©åŠŸèƒ½",
        function_options,
        index=0
    )

    # é€šç”¨å‚æ•°
    col1, col2 = st.columns(2)
    with col1:
        st.session_state.temperature = st.slider("åˆ›æ„åº¦", 0.0, 1.0, 0.5, 0.1)
    with col2:
        st.session_state.max_tokens = st.slider("å“åº”é•¿åº¦", 100, 4096, 2048, 100)

    # è”ç½‘æœç´¢åŠŸèƒ½æŒ‰é’®
    if st.button(
        f"ğŸŒ è”ç½‘æœç´¢[{('on' if st.session_state.search_enabled else 'off')}]",
        use_container_width=True
    ):
        st.session_state.search_enabled = not st.session_state.search_enabled
        st.rerun()

    # RAG åŠŸèƒ½æŒ‰é’®
    if st.button(
        f"ğŸ“š RAG åŠŸèƒ½[{('on' if st.session_state.rag_enabled else 'off')}]",
        use_container_width=True
    ):
        st.session_state.rag_enabled = not st.session_state.rag_enabled
        st.rerun()

    # API æµ‹è¯•åŠŸèƒ½
    st.subheader("API æµ‹è¯•")
    if st.button("ğŸ” æµ‹è¯• API è¿æ¥"):
        if not st.session_state.api_keys:
            st.error("è¯·å…ˆè¾“å…¥ API å¯†é’¥ï¼")
        else:
            with st.spinner("æ­£åœ¨æµ‹è¯• API è¿æ¥..."):
                try:
                    test_prompt = "æ‚¨å¥½ï¼Œè¯·å›å¤'è¿æ¥æˆåŠŸ'ã€‚"
                    response = call_model_api(test_prompt, st.session_state.selected_model)
                    if response:
                        st.success(f"API è¿æ¥æˆåŠŸï¼æ¨¡å‹å›å¤ï¼š{response}")
                    else:
                        st.error("API è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥å¯†é’¥å’Œç½‘ç»œè®¾ç½®ã€‚")
                except Exception as e:
                    st.error(f"API æµ‹è¯•å¤±è´¥ï¼š{str(e)}")

    if st.button("ğŸ§¹ æ¸…ç©ºå¯¹è¯å†å²"):
        st.session_state.messages = []
        st.rerun()

    # åœ¨ä¸»ç•Œé¢çš„ä¾§è¾¹æ æ·»åŠ  ChromaDB é…ç½®
    if st.session_state.rag_enabled:
        configure_chromadb()

    # åœ¨APIæµ‹è¯•åŠŸèƒ½ä¹‹å‰æ·»åŠ åŠ©æ‰‹å¸‚åœºé…ç½®
    with st.expander("ğŸ‘¥ åŠ©æ‰‹å¸‚åœº", expanded=True):
        st.markdown("### ä¸“ä¸šåŠ©æ‰‹é€‰æ‹©")
        
        domain = st.selectbox(
            "é€‰æ‹©é¢†åŸŸ",
            options=list(st.session_state.assistant_market.keys()),
            key="domain_selector"
        )

        assistant = st.selectbox(
            "é€‰æ‹©ä¸“ä¸šåŠ©æ‰‹",
            options=["æ— "] + list(st.session_state.assistant_market[domain].keys()),
            key="assistant_selector"
        )

        if assistant != "æ— ":
            st.session_state.selected_assistant = assistant
            st.markdown(f"**å½“å‰åŠ©æ‰‹è§’è‰²ï¼š** {assistant}")
            # ä½¿ç”¨å¯æŠ˜å çš„å®¹å™¨æ›¿ä»£expander
            with st.container():
                st.markdown("**åŠ©æ‰‹è¯¦ç»†è¯´æ˜ï¼š**")
                st.markdown(st.session_state.assistant_market[domain][assistant])
        else:
            st.session_state.selected_assistant = None

        st.markdown("""
        **ä½¿ç”¨è¯´æ˜ï¼š**
        1. é€‰æ‹©ä¸“ä¸šé¢†åŸŸå’Œå…·ä½“è§’è‰²
        2. åŠ©æ‰‹å°†ä»¥ä¸“ä¸šè§’è‰²èº«ä»½å›ç­”
        3. å¯éšæ—¶åˆ‡æ¢æˆ–å–æ¶ˆè§’è‰²
        4. ä¸“ä¸šå»ºè®®ä»…ä¾›å‚è€ƒ
        """)

    # åœ¨ä¾§è¾¹æ æœ€ä¸‹æ–¹æ·»åŠ æ›´æ–°è¯´æ˜
    # å»ºè®®æ”¾åœ¨æ‰€æœ‰ä¾§è¾¹æ å†…å®¹ä¹‹å
    st.sidebar.markdown("<br>" * 2, unsafe_allow_html=True)  # æ·»åŠ ä¸€äº›ç©ºè¡Œä½œä¸ºé—´éš”
    st.sidebar.markdown("""
    <div style='font-size: 0.8em; color: #666666; border-top: 1px solid #e6e6e6; padding-top: 10px;'>
    <b>æ›´æ–°è¯´æ˜ï¼š</b>
    <br>1. å¢åŠ è…¾è®¯æ··å…ƒå¤§æ¨¡å‹æ”¯æŒ
    <br>2. å¢åŠ "åŠ©æ‰‹å¸‚åœº"åŠŸèƒ½
    <br>3. å¼ºåŒ–æ‰€æœ‰æ¨¡å‹ä¸Šä¸‹æ–‡è®°å¿†åŠŸèƒ½
    </div>
    """, unsafe_allow_html=True)

# ====================
# ä¸»ç•Œé¢å¸ƒå±€
# ====================
st.title("ğŸ¤– å¤šæ¨¡å‹æ™ºèƒ½åŠ©æ‰‹2.10(å­¦æœ¯å¢å¼ºç‰ˆ)")

# æ–‡ä»¶å’Œç½‘å€ä¸Šä¼ åŒºåŸŸ
st.markdown("### ğŸ“ æ–‡ä»¶ä¸Šä¼ ")

# RAG æ¨¡å¼ï¼šå¤šæ–‡ä»¶ä¸Šä¼ å’Œç½‘å€è¾“å…¥
if st.session_state.rag_enabled:
    # æ–‡ä»¶ä¸Šä¼ 
    uploaded_files = st.file_uploader(
        "æ”¯æŒå¤šä¸ªæ–‡ä»¶ä¸Šä¼ ï¼ˆå»ºè®®ä¸è¶…è¿‡5ä¸ªï¼‰",
        accept_multiple_files=True,
        type=["txt", "pdf", "docx", "doc", "csv", "xlsx", "xls"],
        key="multi_file_uploader"
    )
    
    # ç½‘å€è¾“å…¥
    st.markdown("### ğŸ”— ç½‘å€ä¸Šä¼ ")
    urls_input = st.text_area(
        "æ¯è¡Œè¾“å…¥ä¸€ä¸ªç½‘å€ï¼ˆå»ºè®®ä¸è¶…è¿‡5ä¸ªï¼‰",
        height=100,
        key="urls_input",
        placeholder="https://example1.com\nhttps://example2.com"
    )
    
    # æäº¤æŒ‰é’®
    if st.button("ğŸ“¤ æäº¤æ–‡ä»¶å’Œç½‘å€"):
        if not uploaded_files and not urls_input.strip():
            st.warning("è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªæ–‡ä»¶æˆ–è¾“å…¥ä¸€ä¸ªç½‘å€ã€‚")
        else:
            success_count = 0
            # å¤„ç†æ–‡ä»¶
            if uploaded_files:
                if len(uploaded_files) > 5:
                    st.warning("âš ï¸ æ–‡ä»¶æ•°é‡è¶…è¿‡5ä¸ªï¼Œå»ºè®®å‡å°‘æ–‡ä»¶æ•°é‡ä»¥è·å¾—æ›´å¥½çš„å¤„ç†æ•ˆæœã€‚")
                
                for file in uploaded_files:
                    with st.spinner(f"æ­£åœ¨å¤„ç†æ–‡ä»¶ï¼š{file.name}"):
                        try:
                            content = extract_text_from_file(file)
                            if content:
                                if rag_index_document(content, file.name):
                                    success_count += 1
                                    st.session_state.rag_data.append(file.name)
                                    st.success(f"âœ… æ–‡ä»¶ {file.name} å·²æˆåŠŸåŠ å…¥çŸ¥è¯†åº“")
                            else:
                                st.error(f"âŒ æ— æ³•æå–æ–‡ä»¶å†…å®¹ï¼š{file.name}")
                        except Exception as e:
                            st.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")
            
            # å¤„ç†ç½‘å€
            if urls_input.strip():
                process_urls(urls_input)

            if success_count > 0:
                st.success(f"âœ… å…±æˆåŠŸå¤„ç† {success_count} ä¸ªæ–‡ä»¶/ç½‘å€")
                # å¼ºåˆ¶åˆ·æ–°å‘é‡åº“å®ä¾‹
                st.session_state.vector_store = None
                # é‡æ–°åŠ è½½å‘é‡åº“
                get_vector_store()
            else:
                st.error("âŒ æœªèƒ½æˆåŠŸå¤„ç†ä»»ä½•æ–‡ä»¶æˆ–ç½‘å€")

# é RAG æ¨¡å¼ï¼šå•æ–‡ä»¶ä¸Šä¼ å¹¶ç«‹å³å¤„ç†
else:
    uploaded_file = st.file_uploader(
        "ä¸Šä¼ å•ä¸ªæ–‡ä»¶è¿›è¡Œåˆ†æ",
        accept_multiple_files=False,
        type=["txt", "pdf", "docx", "doc", "jpg", "jpeg", "png", "mp3", "wav", "m4a"],
        key="single_file_uploader"
    )
    
    if uploaded_file:
        file_type = uploaded_file.name.split('.')[-1].lower()
        
        try:
            # 1. è¯­éŸ³è¯†åˆ«ï¼ˆGPTsï¼‰
            if file_type in ["mp3", "wav", "m4a"]:
                with st.spinner("ğŸµ æ­£åœ¨è¿›è¡Œè¯­éŸ³è¯†åˆ«..."):
                    if "OpenAI" not in st.session_state.api_keys:
                        st.error("è¯·å…ˆé…ç½® OpenAI API å¯†é’¥")
                    else:
                        client = OpenAI(api_key=st.session_state.api_keys["OpenAI"])
                        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_type}") as tmp_file:
                            tmp_file.write(uploaded_file.getvalue())
                            tmp_file.flush()
                            
                            with open(tmp_file.name, "rb") as audio_file:
                                transcription = client.audio.transcriptions.create(
                                    model="whisper-1",
                                    file=audio_file,
                                    language="zh"
                                )
                        
                        st.success("âœ… è¯­éŸ³è¯†åˆ«å®Œæˆ")
                        with st.chat_message("assistant"):
                            st.markdown(f"**è¯­éŸ³è¯†åˆ«ç»“æœï¼š**\n\n{transcription.text}")
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": f"è¯­éŸ³æ–‡ä»¶ {uploaded_file.name} çš„è¯†åˆ«ç»“æœï¼š\n\n{transcription.text}",
                            "type": "text"
                        })
            
            # 2. å›¾ç‰‡åˆ†æï¼ˆmoonshot-v1-8k-vision-previewï¼‰
            elif file_type in ["jpg", "jpeg", "png"]:
                with st.spinner("ğŸ–¼ï¸ æ­£åœ¨åˆ†æå›¾ç‰‡..."):
                    if st.session_state.selected_model == "Kimi(è§†è§‰ç†è§£)":  # åªä½¿ç”¨ Kimi è¿›è¡Œè§†è§‰ç†è§£
                        if "Kimi(è§†è§‰ç†è§£)" not in st.session_state.api_keys:
                            st.error("è¯·å…ˆé…ç½® Kimi(è§†è§‰ç†è§£) API å¯†é’¥")
                        else:
                            image_content = uploaded_file.getvalue()
                            encoded_image = base64.b64encode(image_content).decode('utf-8')
                            
                            headers = {
                                "Content-Type": "application/json",
                                "Authorization": f"Bearer {st.session_state.api_keys['Kimi(è§†è§‰ç†è§£)']}"
                            }
                            
                            payload = {
                                "model": "moonshot-v1-8k-vision-preview",
                                "messages": [
                                    {
                                        "role": "user",
                                        "content": [
                                            {
                                                "type": "text",
                                                "text": "è¯·è¯¦ç»†åˆ†æè¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦å¯¹è±¡ã€åœºæ™¯ã€ç»†èŠ‚ç­‰æ–¹é¢ã€‚"
                                            },
                                            {
                                                "type": "image_url",
                                                "image_url": {
                                                    "url": f"data:image/jpeg;base64,{encoded_image}"
                                                }
                                            }
                                        ]
                                    }
                                ]
                            }
                            
                            response = requests.post(
                                "https://api.moonshot.cn/v1/chat/completions",
                                json=payload,
                                headers=headers
                            )
                            
                            if response.status_code == 200:
                                result = response.json()["choices"][0]["message"]["content"]
                                st.success("âœ… å›¾ç‰‡åˆ†æå®Œæˆ")
                                with st.chat_message("assistant"):
                                    st.markdown(f"**å›¾ç‰‡åˆ†æç»“æœï¼š**\n\n{result}")
                                st.session_state.messages.append({
                                    "role": "assistant",
                                    "content": f"å›¾ç‰‡ {uploaded_file.name} çš„åˆ†æç»“æœï¼š\n\n{result}",
                                    "type": "text"
                                })
                            else:
                                st.error(f"âŒ å›¾ç‰‡åˆ†æå¤±è´¥ï¼š{response.text}")
            
            # 3. æ–‡æ¡£æ€»ç»“
            elif file_type in ["txt", "pdf", "docx", "doc"]:
                with st.spinner("ğŸ“„ æ­£åœ¨æ€»ç»“æ–‡æ¡£..."):
                    content = extract_text_from_file(uploaded_file)
                    if content:
                        summary_prompt = f"""è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œä¸“ä¸šçš„æ€»ç»“åˆ†æï¼š

æ–‡æœ¬å†…å®¹ï¼š
{content}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œæ€»ç»“ï¼š
1. æ ¸å¿ƒè¦ç‚¹ï¼ˆæœ€é‡è¦çš„2-3ä¸ªå…³é”®ä¿¡æ¯ï¼‰
2. ä¸»è¦å†…å®¹æ¦‚è¿°
3. é‡è¦ç»“è®ºæˆ–å‘ç°
4. ç›¸å…³å»ºè®®ï¼ˆå¦‚æœé€‚ç”¨ï¼‰

è¯·ç”¨æ¸…æ™°ã€ä¸“ä¸šçš„è¯­è¨€ç»„ç»‡å›ç­”ã€‚"""

                        summary = call_model_api(summary_prompt, st.session_state.selected_model)
                        if summary:
                            st.success("âœ… æ–‡æ¡£æ€»ç»“å®Œæˆ")
                            with st.chat_message("assistant"):
                                st.markdown(f"**æ–‡æ¡£æ€»ç»“ç»“æœï¼š**\n\n{summary}")
                            st.session_state.messages.append({
                                "role": "assistant",
                                "content": f"æ–‡æ¡£ {uploaded_file.name} çš„æ€»ç»“ï¼š\n\n{summary}",
                                "type": "text"
                            })
            
            else:
                st.warning(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼š{file_type}")
        
        except Exception as e:
            st.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")
            import traceback
            st.error(f"è¯¦ç»†é”™è¯¯ï¼š{traceback.format_exc()}")

# ====================
# ç”¨æˆ·é—®é¢˜è¾“å…¥åŒºåŸŸ
with st.container():
    # åˆå§‹æç¤ºï¼ˆä»…åœ¨å¯¹è¯è®°å½•ä¸ºç©ºæ—¶æ˜¾ç¤ºï¼‰
    if not st.session_state.messages:
        with st.chat_message("assistant"):
            st.write("æ‚¨å¥½ï¼æˆ‘æ˜¯å¤šæ¨¡å‹æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·é€‰æ‹©æ¨¡å‹å’ŒåŠŸèƒ½å¼€å§‹äº¤äº’ã€‚")
            
    # åœ¨ä¸»ç•Œé¢èŠå¤©éƒ¨åˆ†ï¼Œä¿®æ”¹ç”¨æˆ·è¾“å…¥åŒºåŸŸçš„ä»£ç 
    # åœ¨ç”¨æˆ·è¾“å…¥å‰æ·»åŠ å½“å‰åŠ©æ‰‹æç¤º
    if st.session_state.selected_assistant:
        st.markdown(
            f"<p style='color: #666666; font-size: 0.8em; margin-bottom: 5px;'> ğŸ‘¨ å½“å‰åŠ©æ‰‹ï¼š{st.session_state.selected_assistant}</p>", 
            unsafe_allow_html=True
        )

    # ç”¨æˆ·è¾“å…¥
    user_input = st.chat_input(
        "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜",
        key="user_input"
    )
    
    if user_input:
        # è®°å½•ç”¨æˆ·è¾“å…¥åˆ°å†å²è®°å½•
        manage_chat_history(st.session_state.selected_model, "user", user_input)
        
        with st.chat_message("user"):
            st.write(user_input)
        
        st.session_state.messages.append({"role": "user", "content": user_input, "type": "text"})
        
        with st.spinner("ğŸ§  æ­£åœ¨æ€è€ƒ..."):
            combined_response = ""
            
            # è”ç½‘æœç´¢éƒ¨åˆ†
            if st.session_state.search_enabled:
                try:
                    search_response = get_search_response(user_input)
                    if search_response:
                        combined_response += search_response + "\n\n"
                except Exception as e:
                    st.error(f"æœç´¢è¿‡ç¨‹å‡ºé”™ï¼š{str(e)}")
            
            # RAG æ£€ç´¢éƒ¨åˆ†
            if st.session_state.rag_enabled:
                try:
                    rag_response = rag_generate_response(user_input)
                    if rag_response:
                        combined_response += "ğŸ“š **çŸ¥è¯†åº“æ£€ç´¢ç»“æœï¼š**\n\n" + rag_response + "\n\n"
                except Exception as e:
                    st.error(f"RAG æ£€ç´¢å‡ºé”™ï¼š{str(e)}")
            
            # å¦‚æœä¸¤ä¸ªåŠŸèƒ½éƒ½æœªå¼€å¯ï¼Œä½¿ç”¨æ™®é€šå¯¹è¯æ¨¡å¼
            if not (st.session_state.search_enabled or st.session_state.rag_enabled):
                response = call_model_api(user_input, st.session_state.selected_model)
                if response:
                    combined_response = response
            
            # æ˜¾ç¤ºç»„åˆåçš„å›ç­”
            if combined_response:
                with st.chat_message("assistant"):
                    st.markdown(combined_response)
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": combined_response,
                    "type": "text"
                })
                # è®°å½•åŠ©æ‰‹å›ç­”åˆ°å†å²è®°å½•
                manage_chat_history(st.session_state.selected_model, "assistant", combined_response)
            else:
                st.error("æœªèƒ½è·å–åˆ°ä»»ä½•ç»“æœï¼Œè¯·é‡è¯•ã€‚")

# ====================
# æ˜¾ç¤ºå†å²å¯¹è¯è®°å½•
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        if msg.get("type") == "image":
            st.image(msg["content"])
        else:
            st.write(msg["content"])